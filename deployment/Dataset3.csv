,Questions,Answer,question_clean
0,  What is Linear Regression Algorithm?,"In simple terms: It is a method of finding the best straight line fitting to the given dataset, i.e. tries to find the best linear relationship between the independent and dependent variables.In technical terms: It is a supervised machine learning algorithm that finds the best linear-fit relationship on the given dataset, between independent and dependent variables. It is mostly done with the help of the Sum of Squared Residuals Method, known as the Ordinary least squares (OLS) method.",linear regression algorithm
1,  How do you interpret a linear regression model?,"As we know that the linear regression model is of the form:The significance of the linear regression model lies in the fact that we can easily interpret and understand the marginal changes in the independent variables(predictors) and observed their consequences on the dependent variable(response).Therefore, a linear regression model is quite easy to interpret.For Example, if we increase the value of x1 increases by 1 unit, keeping other variables constant, then the total increase in the value of y will be ÃÂ²i and the intercept term (ÃÂ²0) is the response when all the predictorÃ¢ÂÂs terms are set to zero or not considered.",interpret linear regression model
2,  What are the basic assumptions of the Linear Regression Algorithm?,"The basic assumptions of the Linear regression algorithm are as follows: Linearity: The relationship between the features and target. Homoscedasticity: The error term has a constant variance. Multicollinearity: There is no multicollinearity between the features. Independence: Observations are independent of each other. Normality: The error(residuals) follows a normal distribution. Now, letÃ¢ÂÂs break these assumptions into different categories:It is assumed that there exists a linear relationship between the dependent and the independent variables. Sometimes, this assumption is known as the Ã¢ÂÂlinearity assumptionÃ¢ÂÂ. Normality assumption: The error terms, ÃÂµ(i), are normally distributed. Zero mean assumption: The residuals have a mean value of zero. Constant variance assumption: The residual terms have the same (but unknown) value of variance, ÃÂ2. This assumption is also called the assumption of homogeneity or homoscedasticity. Independent error assumption: The residual terms are independent of each other, i.e. their pair-wise covariance value is zero.  The independent variables are measured without error. There does not exist a linear dependency between the independent variables, i.e. there is no multicollinearity in the data. ",basic assumption linear regression algorithm
3,  Explain the difference between Correlation and Regression.,Correlation: It measures the strength or degree of relationship between two variables. It doesnÃ¢ÂÂt capture causality. It is visualized by a single point.Regression:ÃÂ It measures how one variable affects another variable. Regression is all about model fitting. It tries to capture the causality and describes the cause and the effect. It is visualized by a regression line.,difference correlation regression
4,  Explain the Gradient Descent algorithm with respect to linear regression.,"Gradient descent is a first-order optimization algorithm. In linear regression, this algorithm is used to optimize the cost function to find the values of the ÃÂ²s (estimators) corresponding to the optimized value of the cost function.The working of Gradient descent is similar to a ball that rolls down a graph (ignoring the inertia). In that case, the ball moves along the direction of the maximum gradient and comes to rest at the flat surface i.e, corresponds to minima.Mathematically, the main objective of the gradient descent for linear regression is to find the solution of the following expression,ArgMin J(ÃÂ¸0, ÃÂ¸1), where J(ÃÂ¸0, ÃÂ¸1) represents the cost function of the linear regression. It is given by :Here, h is the linear hypothesis model, defined as h=ÃÂ¸0 + ÃÂ¸1x,y is the target column or output, and m is the number of data points in the training set.Step-1: Gradient Descent starts with a random solution,Step-2: Based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.The updated value for the parameter is given by the formulae:Repeat until convergence(upto minimum loss function)",gradient descent algorithm respect linear regression
5,  Justify the cases where the linear regression algorithm is suitable for a given dataset.,"Generally, a Scatter plot is used to see if linear regression is suitable for any given data. So, we can go for a linear model if the relationship looks somewhat linear. Plotting the scatter plots is easy in the case of simple or univariate linear regression.But if we have more than one independent variable i.e, the case of multivariate linear regression, then two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted to find the suitableness.On the contrary, to make the relationship linear we have to apply some transformations.",justify case linear regression algorithm suitable given dataset
6,  List down some of the metrics used to evaluate a Regression Model.,"Mainly, there are five metrics that are commonly used to evaluate the regression models: Mean Absolute Error(MAE) Mean Squared Error(MSE) Root Mean Squared Error(RMSE) R-Squared(Coefficient of Determination) Adjusted R-Squared ",metric used evaluate regression model
7,"  For a linear regression model, how do we interpret a Q-Q plot?","The Q-Q plot represents a graphical plotting of the quantiles of two distributions with respect to each other. In simple words, we plot quantiles against quantiles in the Q-Q plot which is used to check the normality of errors.Whenever we interpret a Q-Q plot, we should concentrate on the Ã¢ÂÂy = xÃ¢ÂÂ line, which corresponds to a normal distribution. Sometimes, this line is also known as the 45-degree line in statistics.It implies that each of the distributions has the same quantiles. In case you witness a deviation from this line, one of the distributions could be skewed when compared to the other i.e, normal distribution.",linear regression model interpret qq plot
8,"  In linear regression, what is the value of the sum of the residuals for a given dataset? Explain with proper justification.","The sum of the residuals in a linear regression model is 0 since it assumes that the errors (residuals) are normally distributed with an expected value or mean equal to 0, i.e.Y = ÃÂ²T X + ÃÂµHere, Y is the dependent variable or the target column, and ÃÂ² is the vector of the estimates of the regression coefficient,X is the feature matrix containing all the features as the columns, ÃÂµ is the residual term such that ÃÂµ ~ N(0, ÃÂ2).Moreover, the sum of all the residuals is calculated as the expected value of the residuals times the total number of observations in our dataset. Since the expectation of residuals is 0, therefore the sum of all the residual terms is zero.Note: N(ÃÂ¼, ÃÂ2)ÃÂ denotes the standard notation for a normal distribution having mean ÃÂ¼ and standard deviation ÃÂ2.",linear regression value sum residual given dataset proper justification
9,  What are RMSE and MSE? How to calculate it?,"RMSE and MSE are the two of the most common measures of accuracy for linear regression.MSE (Mean Squared Error) is defined as the average of all the squared errors(residuals) for all data points. In simple words, we can say it is an average of squared differences between predicted and actual values.RMSE (Root Mean Squared Error) is the square root of the average of squared differences between predicted and actual values.RMSE stands for Root mean square error, which represented by the formulae:MSE stands for Mean square error, which represented by the formulae:Increment in RMSE is larger than MAE as the test sample size increases. In general, as the variance of error magnitudes increase, MAE remains steady but RMSE increases.",rmse mse calculate
10,  What is OLS?,"OLS stands for Ordinary Least Squares. The main objective of the linear regression algorithm is to find coefficients or estimates by minimizing the error term i.e, the sum of squared errors. This process is known as OLS.This method finds the best fit line, known as regression line by minimizing the sum of square differences between the observed and predicted values.",ols
11,  What are MAE and MAPE?,"MAE stands for Mean Absolute Error, which is defined as the average of absolute or positive errors of all values. In simple words, we can say MAE is an average of absolute or positive differences between predicted values and the actual values.MAPE stands for Mean Absolute Percent Error, which calculates the average absolute error in percentage terms. In simple words, It can be understood as the percentage average of absolute or positive errors.ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  Image Source: Google Images",mae mape
12,  Which evaluation metric should you prefer to use for a dataset having a lot of outliers in it?,"Mean Absolute Error(MAE) is preferred when we have too many outliers present in the dataset because MAE is robust to outliers whereas MSE and RMSE are very susceptible to outliers and these start penalizing the outliers by squaring the error terms, commonly known as residuals.",evaluation metric prefer use dataset lot outlier
13,  Explain the normal form equation of the linear regression.,"The normal equation for linear regression is :ÃÂ²=(XTX)-1XTYThis is also known as the closed-form solution for a linear regression model.where,Y=ÃÂ²TX is the equation that represents the model for the linear regression,Y is the dependent variable or target column,ÃÂ² is the vector of the estimates of the regression coefficient, which is arrived at using the normal equation,X is the feature matrix that contains all the features in the form of columns. The thing to note down here is that the first column in the X matrix consists of all 1s, to incorporate the offset value for the regression line.",normal form equation linear regression
14,  When should it be preferred to the Gradient Descent method instead of the Normal Equation in Linear Regression Algorithm?,"To answer the given question, letÃ¢ÂÂs first understand the difference between the Normal equation and Gradient descent method for linear regression: Needs hyper-parameter tuning for alpha (learning parameter). It is an iterative process. Time complexity- O(kn2) Preferred when n is extremely large.  No such need for any hyperparameter. It is a non-iterative process. Time complexity- O(n3) due to evaluation of XTX. Becomes quite slow for large values of n. where,Ã¢ÂÂkÃ¢ÂÂ represents the maximum number of iterations used for the gradient descent algorithm, andÃ¢ÂÂnÃ¢ÂÂ is the total number of observations present in the training dataset.Clearly, if we have large training data, a normal equation is not preferred for use due to very high time complexity but for small values of Ã¢ÂÂnÃ¢ÂÂ, the normal equation is faster than gradient descent.",preferred gradient descent method instead normal equation linear regression algorithm
15,  What are R-squared and Adjusted R-squared?,"R-square (R2), also known as the coefficient of determination measures the proportion of the variation in your dependent variable (Y) explained by your independent variables (X) for a linear regression model.The main problem with the R-squared is that it will always remain the same or increases as we are adding more independent variables. Therefore, to overcome this problem, an Adjusted-R2 square comes into the picture by penalizing those adding independent variables that do not improve your existing model.To learn more about, R2 and adjusted-R2, refer to the link.",rsquared adjusted rsquared
16,  What are the flaws in R-squared?,"There are two major flaws of R-squared:Problem- 1: As we are adding more and more predictors, RÃÂ² always increases irrespective of the impact of the predictor on the model. As RÃÂ² always increases and never decreases, it can always appear to be a better fit with the more independent variables(predictors) we add to the model. This can be completely misleading.Problem- 2: Similarly, if our model has too many independent variables and too many high-order polynomials, we can also face the problem of over-fitting the data. Whenever the data is over-fitted, it can lead to a misleadingly high RÃÂ² value which eventually can lead to misleading predictions.To learn more about, flaws of R2, refer to the link.",flaw rsquared
17,  What is Multicollinearity?,"It is a phenomenon where two or more independent variables(predictors) are highly correlated with each other i.e. one variable can be linearly predicted with the help of other variables. It determines the inter-correlations and inter-association among independent variables. Sometimes, multicollinearity can also be known as collinearity. Inaccurate use of dummy variables. Due to a variable that can be computed from the other variable in the dataset.  Impacts regression coefficients i.e, coefficients become indeterminate. Causes high standard errors.  By using the correlation coefficient. With the help of Variance inflation factor (VIF), and Eigenvalues. To learn more about, multicollinearity, refer to the link.",multicollinearity
18,  What is Heteroscedasticity? How to detect it?,"It refers to the situation where the variations in a particular independent variable are unequal across the range of values of a second variable that tries to predict it.ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  ÃÂ  Image Source: Google ImagesTo detect heteroscedasticity, we can use graphs or statistical tests such as the Breush-Pagan test and NCV test, etc.",heteroscedasticity detect
19,  What are the disadvantages of the linear regression Algorithm?,"The main disadvantages of linear regression are as follows: Assumption of linearity: It assumes that there exists a linear relationship between the independent variables(input) and dependent variables (output), therefore we are not able to fit the complex problems with the help of a linear regression algorithm. Outliers: It is sensitive to noise and outliers. Multicollinearity: It gets affected by multicollinearity. ",disadvantage linear regression algorithm
20,  What is VIF? How do you calculate it?,"VIF stands for Variance inflation factor, which measures how much variance of an estimated regression coefficient is increased due to the presence of collinearity between the variables. It also determines how much multicollinearity exists in a particular regression model.Firstly, it applies the ordinary least square method of regression that has Xi as a function of all the other explanatory or independent variables and then calculates VIF using the given below mathematical formula:",vif calculate
21,  How is Hypothesis testing used in Linear Regression Algorithm?,"For the following purposes, we can carry out the Hypothesis testing in linear regression:",hypothesis testing used linear regression algorithm
22,  To check whether an independent variable (predictor) is significant or not for the prediction of the target variable. Two common methods for this are Ã¢ÂÂ,"If the p-value of a particular independent variable is greater than a certain threshold (usually 0.05), then that independent variable is insignificant for the prediction of the target variable.If the value of the regression coefficient corresponding to a particular independent variable is zero, then that variable is insignificant for the predictions of the dependent variable and has no linear relationship with it.",check whether independent variable predictor significant prediction target variable two common method ãââ
23,"  To verify whether the calculated regression coefficients i.e, with the help of linear regression algorithm, are good estimators or not of the actual coefficients.",,verify whether calculated regression coefficient ie help linear regression algorithm good estimator actual coefficient
24,  Is it possible to apply Linear Regression for Time Series Analysis?,"Yes, we can apply a linear regression algorithm for doing analysis on time series data, but the results are not promising and hence is not advisable to do so.The reasons behind not preferable linear regression on time-series data are as follows: Time series data is mostly used for the prediction of the future but in contrast, linear regression generally seldom gives good results for future prediction as it is basically not meant for extrapolation. Moreover, time-series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis. Unique Data Visualization Techniques To Make Your Plots Stand Out",possible apply linear regression time series analysis
25,  What are the important assumptions of Linear regression?,"A linear relationshipRestricted Multi-collinearity valueHomoscedasticityFirstly, there has to be a linear relationship between the dependent and the independent variables. To check this relationship, a scatter plot proves to be useful.Secondly, there must no or very little multi-collinearity between the independent variables in the dataset. The value needs to be restricted, which depends on the domain requirement.The third is the homoscedasticity. It is one of the most important assumptions which states that the errors are equally distributed.",important assumption linear regression
26,  What is heteroscedasticity?,"Heteroscedasticity is exactly the opposite of homoscedasticity, which means that the error terms are not equally distributed. To correct this phenomenon, usually, a log function is used.",heteroscedasticity
27,  What is the difference between R square and adjusted R square?,"R square and adjusted R square values are used for model validation in case of linear regression. R square indicates the variation of all the independent variables on the dependent variable. i.e. it considers all the independent variable to explain the variation. In the case of Adjusted R squared, it considers only significant variables(P values less than 0.05) to indicate the percentage of variation in the model.",difference r square adjusted r square
28,  How to find RMSE and MSE?,"RMSE and MSE are the two of the most common measures of accuracy for a linear regression.RMSE indicates the Root mean square error, which indicated by the formulae:Where MSE indicates the Mean square error represented by the formulae:",find rmse mse
29,  What are the possible ways of improving the accuracy of a linear regression model?,"There could be multiple ways of improving the accuracy of a linear regression, most commonly used ways are as follows:-Regression is sensitive to outliers, hence it becomes very important to treat the outliers with appropriate values. Replacing the values with mean, median, mode or percentile depending on the distribution can prove to be useful.",possible way improving accuracy linear regression model
30,  How to interpret a Q-Q plot in a Linear regression model?,"A Q-Q plot is used to check the normality of errors. In the above chart mentioned, Majority of the data follows a normal distribution with tails curled. This shows that the errors are mostly normally distributed but some observations may be due to significantly higher/lower values are affecting the normality of errors.",interpret qq plot linear regression model
31,  What is the significance of an F-test in a linear model?,"Ã¢ÂÂ The use of F-test is to test the goodness of the model. When the model is re-iterated to improve the accuracy with changes, the F-test values prove to be useful in terms of understanding the effect of overall regression.",significance ftest linear model
32, ÃÂ What are the disadvantages of the linear model?,Ã¢ÂÂ Linear regression is sensitive to outliers which may affect the result.Ã¢ÂÂ Over-fittingÃ¢ÂÂ Under-fitting,ãâ disadvantage linear model
33,  What is linear regression?,"In simple terms, linear regression is a method of finding the best straight line fitting to the given data, i.e. finding the best linear relationship between the independent and dependent variables.  In technical terms, linear regression is a machine learning algorithm that finds the best linear-fit relationship on any given data, between independent and dependent variables. It is mostly done by the Sum of Squared Residuals Method.",linear regression
34,  What is the use of regularisation? Explain L1 and L2 regularisations.,"Regularisation is a technique that is used to tackle the problem of overfitting of the model. When a very complex model is implemented on the training data, it overfits. At times, the simple model might not be able to generalise the data and the complex model overfits. To address this problem, regularisation is used.  Regularisation is nothing but adding the coefficient terms (betas) to the cost function so that the terms are penalised and are small in magnitude. This essentially helps in capturing the trends in the data and at the same time prevents overfitting by not letting the model become too complex. ",use regularisation l1 l2 regularisation
35,  How to choose the value of the parameter learning rate (ÃÂ±)?,"Selecting the value of learning rate is a tricky business. If the value is too small, the gradient descent algorithm takes ages to converge to the optimal solution. On the other hand, if the value of the learning rate is high, the gradient descent will overshoot the optimal solution and most likely never converge to the optimal solution. To overcome this problem, you can try different values of alpha over a range of values and plot the cost vs the number of iterations. Then, based on the graphs, the value corresponding to the graph showing the rapid decrease can be chosen.  The aforementioned graph is an ideal cost vs the number of iterations curve. Note that the cost initially decreases as the number of iterations increases, but after certain iterations, the gradient descent converges and the cost does not decrease anymore.  If you see that the cost is increasing with the number of iterations, your learning rate parameter is high and it needs to be decreased. ",choose value parameter learning rate ãâ
36,  How to choose the value of the regularisation parameter (ÃÂ»)?,"Selecting the regularisation parameter is a tricky business. If the value ofÃÂ ÃÂ»ÃÂ is too high, it will lead to extremely small values of the regression coefficientÃÂ ÃÂ², which will lead to the model underfitting (high bias Ã¢ÂÂ low variance). On the other hand, if the value ofÃÂ ÃÂ»ÃÂ is 0 (very small), the model will tend to overfit the training data (low bias Ã¢ÂÂ high variance). There is no proper way to select the value of ÃÂ». What you can do is have a sub-sample of data and run the algorithm multiple times on different sets. Here, the person has to decide how much variance can be tolerated. Once the user is satisfied with the variance, that value ofÃÂ ÃÂ»ÃÂ can be chosen for the full dataset.  One thing to be noted is that the value of ÃÂ»ÃÂ selected here was optimal for that subset, not for the entire training data. ",choose value regularisation parameter ãâ
37,  What value is the sum of the residuals of a linear regression close to? Justify.,"Ans The sum of the residuals of a linear regression is 0. Linear regression works on the assumption that the errors (residuals) are normally distributed with a mean of 0, i.e. Y = ÃÂ²TÃÂ X + ÃÂµHere, Y is the target or dependent variable,  ÃÂ²ÃÂ is the vector of the regression coefficient, X is the feature matrix containing all the features as the columns,  ÃÂµ is the residual term such thatÃÂ ÃÂµ ~ N(0,ÃÂ2).  So, the sum of all the residuals is the expected value of the residuals times the total number of data points. Since the expectation of residuals is 0, the sum of all the residual terms is zero.  Note: N(ÃÂ¼,ÃÂ2) is the standard notation for a normal distribution having mean ÃÂ¼ and standard deviation ÃÂ2.",value sum residual linear regression close justify
38,  How does multicollinearity affect the linear regression?,"Ans Multicollinearity occurs when some of the independent variables are highly correlated (positively or negatively) with each other. This multicollinearity causes a problem as it is against the basic assumption of linear regression. The presence of multicollinearity does not affect the predictive capability of the model. So, if you just want predictions, the presence of multicollinearity does not affect your output. However, if you want to draw some insights from the model and apply them in, letÃ¢ÂÂs say, some business model, it may cause problems. One of the major problems caused by multicollinearity is that it leads to incorrect interpretations and provides wrong insights. The coefficients of linear regression suggest the mean change in the target value if a feature is changed by one unit. So, if multicollinearity exists, this does not hold true as changing one feature will lead to changes in the correlated variable and consequent changes in the target variable. This leads to wrong insights and can produce hazardous results for a business.  A highly effective way of dealing with multicollinearity is the use of VIF (Variance Inflation Factor). Higher the value of VIF for a feature, more linearly correlated is that feature. Simply remove the feature with very high VIF value and re-train the model on the remaining dataset. ",multicollinearity affect linear regression
39,"  You run your regression on different subsets of your data, and in each subset, the beta value for a certain variable varies wildly. What could be the issue here?","This case implies that the dataset is heterogeneous. So, to overcome this problem, the dataset should be clustered into different subsets, and then separate models should be built for each cluster. Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogeneous data quite efficiently.",run regression different subset data subset beta value certain variable varies wildly could issue
40,  Your linear regression doesnÃ¢ÂÂt run and communicates that there is an infinite number of best estimates for the regression coefficients. What could be wrong?,"This condition arises when there is a perfect correlation (positive or negative) between some variables. In this case, there is no unique value for the coefficients, and hence, the given condition arises.",linear regression doesnãâât run communicates infinite number best estimate regression coefficient could wrong
41,  What do you mean by adjusted R2? How is it different from R2?,"Adjusted R2, just like R2, is a representative of the number of points lying around the regression line. That is, it shows how well the model is fitting the training data. The formula for adjusted R2ÃÂ ÃÂ is Ã¢ÂÂ   Here, n is the number of data points, and k is the number of features. One drawback of R2ÃÂ is that it will always increase with the addition of a new feature, whether the new feature is useful or not. The adjusted R2ÃÂ overcomes this drawback. The value of the adjusted R2ÃÂ increases only if the newly added feature plays a significant role in the model.",mean adjusted r2 different r2
42,  How do you interpret the residual vs fitted value curve?,"The residual vs fitted value plot is used to see whether the predicted values and residuals have a correlation or not. If the residuals are distributed normally, with a mean around the fitted value and a constant variance, our model is working fine; otherwise, there is some issue with the model. The most common problem that can be found when training the model over a large range of a dataset is heteroscedasticity(this is explained in the answer below). The presence of heteroscedasticity can be easily seen by plotting the residual vs fitted value curve.",interpret residual v fitted value curve
43,"  What is heteroscedasticity? What are the consequences, and how can you overcome it?","A random variable is said to be heteroscedastic when different subpopulations have different variabilities (standard deviation).  The existence of heteroscedasticity gives rise to certain problems in the regression analysis as the assumption says that error terms are uncorrelated and, hence, the variance is constant. The presence of heteroscedasticity can often be seen in the form of a cone-like scatter plot for residual vs fitted values.  One of the basic assumptions of linear regression is that heteroscedasticity is not present in the data. Due to the violation of assumptions, the Ordinary Least Squares (OLS) estimators are not the Best Linear Unbiased Estimators (BLUE). Hence, they do not give theÃÂ least variance than other Linear Unbiased Estimators (LUEs). There is no fixed procedure to overcome heteroscedasticity. However, there are some ways that may lead to a reduction of heteroscedasticity. They are Ã¢ÂÂ ",heteroscedasticity consequence overcome
44,  What is VIF? How do you calculate it?,"Variance Inflation Factor (VIF) is used to check the presence of multicollinearity in a dataset. It is calculated asÃ¢ÂÂÃÂ   Here, VIFj ÃÂ is the value of VIF for the jthÃÂ variable, Rj2ÃÂ is the R2ÃÂ value of the model when that variable is regressed against all the other independent variables.  If the value of VIF is high for a variable, it implies that the R2ÃÂ ÃÂ value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables.",vif calculate
45,  How do you know that linear regression is suitable for any given data?,"To see if linear regression is suitable for any given data, a scatter plot can be used. If the relationship looks linear, we can go for a linear model. But if it is not the case, we have to apply some transformations to make the relationship linear. Plotting the scatter plots is easy in case of simple or univariate linear regression. But in case of multivariate linear regression, two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted.",know linear regression suitable given data
46,  Explain gradient descent with respect to linear regression.,"Gradient descent is an optimisation algorithm. In linear regression, it is used to optimise the cost function and find the values of the ÃÂ²s (estimators) corresponding to the optimised value of the cost function. Gradient descent works like a ball rolling down a graph (ignoring the inertia). The ball moves along the direction of the greatest gradient and comes to rest at the flat surface (minima).  Mathematically, the aim of gradient descent for linear regression is to find the solution of ArgMin J(ÃÂ0,ÃÂ1), where J(ÃÂ0,ÃÂ1) is the cost function of the linear regression. It is given by Ã¢ÂÂ ÃÂ   Here, h is the linear hypothesis model, h=ÃÂ0ÃÂ + ÃÂ1x,ÃÂ y is the true output,ÃÂ and m is the number of the data points in the training set. Gradient Descent starts with a random solution, and then based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value. The update is: Repeat until convergence ",gradient descent respect linear regression
47,  What is robust regression?,"A regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers.  A regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. ",robust regression
48,  Which graphs are suggested to be observed before model fitting?,"Before fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.",graph suggested observed model fitting
49,  What is the generalized linear model?,The generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.,generalized linear model
50,  Explain the bias-variance trade-off.,"Bias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have aÃÂ low bias. Variance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance. For a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance. There is no escapingÃÂ the relationship between bias and variance in machine learning.So, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.",biasvariance tradeoff
51,  How can learning curves help create a better model?,"Learning curves give the indication of the presence of overfitting or underfitting.  In a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:  If the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias.  If there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from aÃÂ high variance.  Machine Learning Engineers: Myths vs. Realities ThatÃ¢ÂÂs the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments. Co-authored by Ã¢ÂÂ Ojas Agarwal isSingle = true; catStr = 'Artificial Intelligence';  Lead the AI Driven Technological Revolution   										PG Diploma in Machine Learning and Artificial Intelligence									 Learn More   ",learning curve help create better model
52,  What is the difference between a Perceptron and Logistic Regression?,"A Multi-Layer Perceptron (MLP) is one of the most basic neural networks that we use for classification. For a binary classification problem, we know that the output can be either 0 or 1. This is just like our simple logistic regression, where we use a logit function to generate a probability between 0 and 1.So, whatÃ¢ÂÂs the difference between the two?Simply put, it is just the difference in the threshold function! When we restrict the logistic regression model to give us either exactly 1 or exactly 0, we get a Perceptron model:",difference perceptron logistic regression
53,  Can we have the same bias for all neurons of a hidden layer?,"Essentially, you can have a different bias value at each layer or at each neuron as well. However, it is best if we have a bias matrix for all the neurons in the hidden layers as well.A point to note is that both these strategies would give you very different results.",bias neuron hidden layer
54,"  What if we do not use any activation function(s) in a neural network?
","The main aim of this question is to understand why we need activation functions in a neural network. You can start off by giving a simple explanation of how neural networks are built:Step 1: Calculate the sum of all the inputs (X) according to their weights and include the bias term:Z = (weights * X) + biasStep 2: Apply an activation function to calculate the expected output:Y = Activation(Z)Steps 1 and 2 are performed at each layer. If you recollect, this is nothing but forward propagation! Now, what if there is no activation function?Our equation for Y essentially becomes:Y = Z = (weights * X) + biasWait Ã¢ÂÂ isnÃ¢ÂÂt this just a simple linear equation? Yes Ã¢ÂÂ and that is why we need activation functions. A linear equation will not be able to capture the complex patterns in the data Ã¢ÂÂ this is even more evident in the case of deep learning problems.In order to capture non-linear relationships, we use activation functions, and that is why a neural network without an activation function is just a linear regression model.",use activation function neural network
55,"  In a neural network, what if all the weights are initialized with the same value?","In simplest terms, if all the neurons have the same value of weights, each hidden unit will get exactly the same signal. While this might work during forward propagation, the derivative of the cost function during backward propagation would be the same every time.In short, there is no learning happening by the network! What do you call the phenomenon of the model being unable to learn any patterns from the data? Yes, underfitting.Therefore, if all weights have the same initial value, this would lead to underfitting.Note: This question might further lead to questions on exploding and vanishing gradients, which I have covered below.",neural network weight initialized value
56,  List the supervised and unsupervised tasks in Deep Learning.,"Now, this can be one tricky question. There might be a misconception that deep learning can only solve unsupervised learning problems. This is not the case. Some example of Supervised Learning and Deep learning include:On the other hand, there are some unsupervised deep learning techniques as well:Here is a great article on applications of Deep Learning for unsupervised tasks:",supervised unsupervised task deep learning
57,  What is the role of weights and bias in a neural network?,"This is a question best explained with a real-life example. Consider that you want to go out today to play a cricket match with your friends. Now, a number of factors can affect your decision-making, like:And so on. These factors can change your decision greatly or not too much. For example, if it is raining outside, then you cannot go out to play at all. Or if you have only one bat, you can share it while playing as well. The magnitude by which these factors can affect the game is called the weight of that factor.Factors like the weather or temperature might have a higher weight, and other factors like equipment would have a lower weight.However, does this mean that we can play a cricket match with only one bat? No Ã¢ÂÂ we would need 1 ball and 6 wickets as well. This is where bias comes into the picture. Bias lets you assign some threshold which helps you activate a decision-point (or a neuron) only when that threshold is crossed.",role weight bias neural network
58,  How does forward propagation and backpropagation work in deep learning?,"Now, this can be answered in two ways. If you are on a phone interview, you cannot perform all the calculus in writing and show the interviewer. In such cases, it best to explain it as such:For an in-person interview, it is best to take up the marker, create a simple neural network with 2 inputs, a hidden layer, and an output layer, and explain it.Forward propagation:Backpropagation:At layer L2, for all weights:At layer L1, for all weights:You need not explain with respect to the bias term as well, though you might need to expand the above equations substituting the actual derivatives.",forward propagation backpropagation work deep learning
59,  What are the common data structures used in Deep Learning?,"Deep Learning goes right from the simplest data structures like lists to complicated ones like computation graphs.Here are the most common ones:Once the basics are out of the way, the interview would lead to slightly advanced deep learning concepts. These questions are much easier to answer if you have considerable practice with not only the mathematical concepts but also with coding them.Additionally, these questions can also become more project-specific. As a general rule of thumb, it is best to include examples of how you have used the concept asked in the question in your own projects. This has two advantages:Here, I have given an overview of the key concepts in the questions Ã¢ÂÂ you can always customize your answers to add more about your experiences with some of these deep learning algorithms and techniques.",common data structure used deep learning
60,  Why should we use Batch Normalization?,"Once the interviewer has asked you about the fundamentals of deep learning architectures, they would move on to the key topic of improving your deep learning modelÃ¢ÂÂs performance.Batch Normalization is one of the techniques used for reducing the training time of our deep learning algorithm. Just like normalizing our input helps improve our logistic regression model, we can normalize the activations of the hidden layers in our deep learning model as well:We basically normalize a[1] and a[2] here. This means we normalize the inputs to the layer, and then apply the activation functions to the normalized inputs.Here is an article that explains Batch Normalization and other techniques for improving Neural Networks: Neural Networks Ã¢ÂÂ Hyperparameter Tuning, Regularization & Optimization.",use batch normalization
61,  List the activation functions you have used so far in your projects and how you would choose one.,"The most common activation functions are:While it is not important to know all the activation functions, you can always score points by knowing the range of these functions and how they are used. Here is a handy table for you to follow:Here is a great guide on how to use these and other activations functions: Fundamentals of Deep Learning Ã¢ÂÂ Activation Functions and When to Use Them?.",activation function used far project would choose one
62,  Why does a Convolutional Neural Network (CNN) work better with image data?,"The key to this question lies in the Convolution operation. Unlike humans, the machine sees the image as a matrix of pixel values. Instead of interpreting a shape like a petal or an ear, it just identifies curves and edges.Thus, instead of looking at the entire image, it helps to just read the image in parts. Doing this for a 300 x 300 pixel image would mean dividing the matrix into smaller 3 x 3 matrices and dealing with them one by one. This is convolution.Mathematically, we just perform a small operation on the matrix to help us detect features in the image Ã¢ÂÂ like boundaries, colors, etc.Z = X * fHere, we are convolving (* operation Ã¢ÂÂ not multiplication) the input matrix X with another small matrix f, called the kernel/filter to create a new matrix Z. This matrix is then passed on to the other layers.If you have a board/screen in front of you, you can always illustrate this with a simple example:Learning more about how CNNs work here.",convolutional neural network cnn work better data
63,  Why do RNNs work better with text data?,"The main component that differentiates Recurrent Neural Networks (RNN) from the other models is the addition of a loop at each node. This loop brings the recurrence mechanism in RNNs. In a basic Artificial Neural Network (ANN), each input is given the same weight and fed to the network at the same time. So, for a sentence like Ã¢ÂÂI saw the movie and hated itÃ¢ÂÂ, it would be difficult to capture the information which associates Ã¢ÂÂitÃ¢ÂÂ with the Ã¢ÂÂmovieÃ¢ÂÂ.The addition of a loop is to denote preserving the previous nodeÃ¢ÂÂs information for the next node, and so on. This is why RNNs are much better for sequential data, and since text data also is sequential in nature, they are an improvement over ANNs.",rnns work better text data
64,"  In a CNN, if the input size 5 X 5 and the filter size is 7 X 7, then what would be the size of the output?","This is a pretty intuitive answer. As we saw above, we perform the convolution on Ã¢ÂÂxÃ¢ÂÂ one step at a time, to the right, and in the end, we got Z with dimensions 2 X 2, for X with dimensions 3 X 3.Thus, to make the input size similar to the filter size, we make use of padding Ã¢ÂÂ adding 0s to the input matrix such that its new size becomes at least 7 X 7. Thus, the output size would be using the formula:Dimension of image = (n, n) = 5 X 5Dimension of filter = (f,f)ÃÂ  = 7 X 7Padding = 1 (adding 1 pixel with value 0 all around the edges)Dimension of output will be (n+2p-f+1) X (n+2p-f+1) = 1 X 1",cnn input size 5 x 5 filter size 7 x 7 would size output
65,  WhatÃ¢ÂÂs the difference between valid and same padding in a CNN?,"This question has more chances of being a follow-up question to the previous one. Or if you have explained how you used CNNs in a computer vision task, the interviewer might ask this question along with the details of the padding parameters.",whatãââs difference valid padding cnn
66,  What do you mean by exploding and vanishing gradients?,"The key here is to make the explanation as simple as possible. As we know, the gradient descent algorithm tries to minimize the error by taking small steps towards the minimum value. These steps are used to update the weights and biases in a neural network.However, at times, the steps become too large and this results in larger updates to weights and bias terms Ã¢ÂÂ so much so as to cause an overflow (or a NaN) value in the weights. This leads to an unstable algorithm and is called an exploding gradient.On the other hand, the steps are too small and this leads to minimal changes in the weights and bias terms Ã¢ÂÂ even negligible changes at times. We thus might end up training a deep learning model with almost the same weights and biases each time and never reach the minimum error function. This is called the vanishing gradient.A point to note is that both these issues are specifically evident in Recurrent Neural Networks Ã¢ÂÂ so be prepared for follow-up questions on RNN!",mean exploding vanishing gradient
67,  What are the applications of transfer learning in Deep Learning?,"I am sure you would have a doubt as to why a relatively simple question was included in the Intermediate Level. The reason is the sheer volume of subsequent questions it can generate!The use of transfer learning has been one of the key milestones in deep learning. Training a large model on a huge dataset, and then using the final parameters on smaller simpler datasets has led to defining breakthroughs in the form of Pretrained Models. Be it Computer Vision or NLP, pretrained models have become the norm in research and in the industry.Some popular examples include BERT, ResNet, GPT-2, VGG-16, etc and many more.It is here that you can earn brownie points by pointing out specific examples/projects where you used these models and how you used them as well.It is not possible to discuss all of them, so here are a few resources to get started:It is here that questions become really specific to your projects or to what you have discussed in the interview before.Also, depending on the domain Ã¢ÂÂ with Computer Vision or Natural Language Processing, these questions can change. While it is not important to know the architecture of each model in detail, you would need to know the intuition behind them and why these models were needed in the first place.Again, just like the intermediate level, it is important to always bring in examples that you have studied or implemented yourself into the discussion.",application transfer learning deep learning
68,  How backpropagation is different in RNN compared to ANN?,"In Recurrent Neural Networks, we have an additional loop at each node:This loop essentially includes a time component into the network as well. This helps in capturing sequential information from the data, which could not be possible in a generic artificial neural network.This is why the backpropagation in RNN is called Backpropagation through Time, as in backpropagation at each time step.You can find a detailed explanation of RNNs here: Fundamentals of Deep Learning Ã¢ÂÂ Introduction to Recurrent Neural Networks.",backpropagation different rnn compared ann
69,  How does LSTM solve the vanishing gradient challenge?,"The LSTM model is considered a special case of RNNs. The problems of vanishing gradients and exploding gradients we saw earlier are a disadvantage while using the plain RNN model.In LSTMs, we add a forget gate, which is basically a memory unit that retains information that is retained across timesteps and discards the other information that is not needed. This also necessitates the need for input and output gates to include the results of the forget gate as well.",lstm solve vanishing gradient challenge
70,  Why is GRU faster as compared to LSTM?,"As you can see, the LSTM model can become quite complex. In order to still retain the functionality of retaining information across time and yet not make a too complex model, we need GRUs.Basically, in GRUs, instead of having an additional Forget gate, we combine the input and Forget gates into a single Update Gate:It is this reduction in the number of gates that makes GRU less complex and faster than LSTM. You can learn about GRUs, LSTMs and other sequence models in detail here: Must-Read Tutorial to Learn Sequence Modeling & Attention Models.",gru faster compared lstm
71,  How is the transformer architecture better than RNN?,"Advancements in deep learning have made it possible to solve many tasks in Natural Language Processing. Networks/Sequence models like RNNs, LSTMs, etc. are specifically used for this purpose Ã¢ÂÂ so as to capture all possible information from a given sentence, or a paragraph. However, sequential processing comes with its caveats:This gave rise to the Transformer architecture. Transformers use what is called the attention mechanism. This basically means mapping dependencies between all the parts of a sentence.Here is an excellent article explaining transformers: How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models.",transformer architecture better rnn
72,  Describe a project you worked on and the tools/frameworks you used?,"Now, this is one question that is sure to be asked even if none of the above ones is asked in your deep learning interview. I have included it in the advanced section since you might be grilled on each and every part of the code you have written.Before the interview, make sure to:When you are asked such a question, it is best to give a small 30-second pitch on what was the:After this, you can start going into detail about the model architecture, what preprocessing steps you had to take, and how that changed the data.An important point to be noted is that the project need not be a very complicated or sophisticated one. A well-explained object detection project would earn you more points than a poorly-explained video classification project. Towards this end, I recommend having a README file in the above format for every project that you have implemented.Unique Data Visualization Techniques To Make Your Plots Stand Out",project worked toolsframeworks used
73, What is deep learning?,"Deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network. In the mid-1960s, Alexey Grigorevich Ivakhnenko published the first general, while working on deep learning network. Deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.",deep learning
74," What are the main differences between AI, Machine Learning, and Deep Learning?"," AI stands for Artificial Intelligence. It is a technique which enables machines to mimic human behavior. Machine Learning is a subset of AI which uses statistical methods to enable machines to improve with experiences.  Deep learning is a part of Machine learning, which makes the computation of multi-layer neural networks feasible. It takes advantage of neural networks to simulate human-like decision making. ",main difference ai machine learning deep learning
75, Differentiate supervised and unsupervised deep learning procedures.," Supervised learning is a system in which both input and desired output data are provided. Input and output data are labeled to provide a learning basis for future data processing. Unsupervised procedure does not need labeling information explicitly, and the operations can be carried out without the same. The common unsupervised learning method is cluster analysis. It is used for exploratory data analysis to find hidden patterns or grouping in data. ",differentiate supervised unsupervised deep learning procedure
76, What are the applications of deep learning?,There are various applications of deep learning: Computer vision Natural language processing and pattern recognition Image recognition and processing Machine translation Sentiment analysis Question Answering system Object Classification and Detection Automatic Handwriting Generation Automatic Text Generation. ,application deep learning
77, Do you think that deep network is better than a shallow one?,"Both shallow and deep networks are good enough and capable of approximating any function. But for the same level of accuracy, deeper networks can be much more efficient in terms of computation and number of parameters. Deeper networks can create deep representations. At every layer, the network learns a new, more abstract representation of the input.",think deep network better shallow one
78," What do you mean by ""overfitting""?",Overfitting is the most common issue which occurs in deep learning. It usually occurs when a deep learning algorithm apprehends the sound of specific data. It also appears when the particular algorithm is well suitable for the data and shows up when the algorithm or model represents high variance and low bias.,mean overfitting
79, What is Backpropagation?,Backpropagation is a training algorithm which is used for multilayer neural networks. It transfers the error information from the end of the network to all the weights inside the network. It allows the efficient computation of the gradient.Backpropagation can be divided into the following steps: It can forward propagation of training data through the network to generate output. It uses target value and output value to compute error derivative concerning output activations. It can backpropagate to compute the derivative of the error concerning output activations in the previous layer and continue for all hidden layers. It uses the previously calculated derivatives for output and all hidden layers to calculate the error derivative concerning weights. It updates the weights. ,backpropagation
80, What is the function of the Fourier Transform in Deep Learning?,"Fourier transform package is highly efficient for analyzing, maintaining, and managing a large databases. The software is created with a high-quality feature known as the special portrayal. One can effectively utilize it to generate real-time array data, which is extremely helpful for processing all categories of signals.",function fourier transform deep learning
81, Describe the theory of autonomous form of deep learning in a few words.,"There are several forms and categories available for the particular subject, but the autonomous pattern represents independent or unspecified mathematical bases which are free from any specific categorizer or formula.",theory autonomous form deep learning word
82," What is the use of Deep learning in today's age, and how is it adding data scientists?",Deep learning has brought significant changes or revolution in the field of machine learning and data science. The concept of a complex neural network (CNN) is the main center of attention for data scientists. It is widely taken because of its advantages in performing next-level machine learning operations. The advantages of deep learning also include the process of clarifying and simplifying issues based on an algorithm due to its utmost flexible and adaptable nature. It is one of the rare procedures which allow the movement of data in independent pathways. Most of the data scientists are viewing this particular medium as an advanced additive and extended way to the existing process of machine learning and utilizing the same for solving complex day to day issues.,use deep learning today age adding data scientist
83, What are the deep learning frameworks or tools?,"Deep learning frameworks or tools are:Tensorflow, Keras, Chainer, Pytorch, Theano & Ecosystem, Caffe2, CNTK, DyNetGensim, DSSTNE, Gluon, Paddle, Mxnet, BigDL",deep learning framework tool
84, What are the disadvantages of deep learning?,"There are some disadvantages of deep learning, which are: Deep learning model takes longer time to execute the model. In some cases, it even takes several days to execute a single model depends on complexity. The deep learning model is not good for small data sets, and it fails here. ",disadvantage deep learning
85, What is the meaning of term weight initialization in neural networks?,"In neural networking, weight initialization is one of the essential factors. A bad weight initialization prevents a network from learning. On the other side, a good weight initialization helps in giving a quicker convergence and a better overall error. Biases can be initialized to zero. The standard rule for setting the weights is to be close to zero without being too small.",meaning term weight initialization neural network
86, Explain Data Normalization.,"Data normalization is an essential preprocessing step, which is used to rescale values to fit in a specific range. It assures better convergence during backpropagation. In general, data normalization boils down to subtracting the mean of each data point and dividing by its standard deviation.",data normalization
87, Why is zero initialization not a good weight initialization process?,"If the set of weights in the network is put to a zero, then all the neurons at each layer will start producing the same output and the same gradients during backpropagation.As a result, the network cannot learn at all because there is no source of asymmetry between neurons. That is the reason why we need to add randomness to the weight initialization process.",zero initialization good weight initialization process
88, What are the prerequisites for starting in Deep Learning?,"There are some basic requirements for starting in Deep Learning, which are: Machine Learning Mathematics Python Programming ",prerequisite starting deep learning
89, What are the supervised learning algorithms in Deep learning?, Artificial neural network Convolution neural network Recurrent neural network ,supervised learning algorithm deep learning
90, What are the unsupervised learning algorithms in Deep learning?, Self Organizing Maps Deep belief networks (Boltzmann Machine) Auto Encoders ,unsupervised learning algorithm deep learning
91, How many layers in the neural network?, Input Layer The input layer contains input neurons which send information to the hidden layer. Hidden Layer The hidden layer is used to send data to the output layer. Output Layer The data is made available at the output layer. ,many layer neural network
92, What is the use of the Activation function?,"The activation function is used to introduce nonlinearity into the neural network so that it can learn more complex function. Without the Activation function, the neural network would be only able to learn function, which is a linear combination of its input data.Activation function translates the inputs into outputs. The activation function is responsible for deciding whether a neuron should be activated or not. It makes the decision by calculating the weighted sum and further adding bias with it. The basic purpose of the activation function is to introduce non-linearity into the output of a neuron.",use activation function
93, How many types of activation function are available?, Binary Step Sigmoid Tanh ReLU Leaky ReLU Softmax Swish ,many type activation function available
94, What is a binary step function?,"The binary step function is an activation function, which is usually based on a threshold. If the input value is above or below a particular threshold limit, the neuron is activated, then it sends the same signal to the next layer. This function does not allow multi-value outputs.",binary step function
95, What is the sigmoid function?,"The sigmoid activation function is also called the logistic function. It is traditionally a trendy activation function for neural networks. The input data to the function is transformed into a value between 0.0 and 1.0. Input values that are much larger than 1.0 are transformed to the value 1.0. Similarly, values that are much smaller than 0.0 are transformed into 0.0. The shape of the function for all possible inputs is an S-shape from zero up through 0.5 to 1.0. It was the default activation used on neural networks, in the early 1990s.",sigmoid function
96, What is Tanh function?,"The hyperbolic tangent function, also known as tanh for short, is a similar shaped nonlinear activation function. It provides output values between -1.0 and 1.0. Later in the 1990s and through the 2000s, this function was preferred over the sigmoid activation function as models. It was easier to train and often had better predictive performance.",tanh function
97, What is ReLU function?,"A node or unit which implements the activation function is referred to as a rectified linear activation unit or ReLU for short. Generally, networks that use the rectifier function for the hidden layers are referred to as rectified networks.Adoption of ReLU may easily be considered one of the few milestones in the deep learning revolution.",relu function
98, What is the use of leaky ReLU function?,The Leaky ReLU (LReLU or LReL) manages the function to allow small negative values when the input is less than zero.,use leaky relu function
99, What is the softmax function?,"The softmax function is used to calculate the probability distribution of the event over 'n' different events. One of the main advantages of using softmax is the output probabilities range. The range will be between 0 to 1, and the sum of all the probabilities will be equal to one. When the softmax function is used for multi-classification model, it returns the probabilities of each class, and the target class will have a high probability.",softmax function
100, What is a Swish function?,"Swish is a new, self-gated activation function. Researchers at Google discovered the Swish function. According to their paper, it performs better than ReLU with a similar level of computational efficiency. ",swish function
101, What is the most used activation function?,Relu function is the most used activation function. It helps us to solve vanishing gradient problems.,used activation function
102, Can Relu function be used in output layer?,"No, Relu function has to be used in hidden layers.",relu function used output layer
103, In which layer softmax activation function used?,Softmax activation function has to be used in the output layer.,layer softmax activation function used
104, What do you understand by Autoencoder?,"Autoencoder is an artificial neural network. It can learn representation for a set of data without any supervision. The network automatically learns by copying its input to the output; typically,internet representation consists of smaller dimensions than the input vector. As a result, they can learn efficient ways of representing the data. Autoencoder consists of two parts; an encoder tries to fit the inputs to the internal representation, and a decoder converts the internal state to the outputs.",understand autoencoder
105, What do you mean by Dropout?,"Dropout is a cheap regulation technique used for reducing overfitting in neural networks. We randomly drop out a set of nodes at each training step. As a result, we create a different model for each training case, and all of these models share weights. It's a form of model averaging.",mean dropout
106, What do you understand by Tensors?,"Tensors are nothing but a de facto for representing the data in deep learning. They are just multidimensional arrays, which allows us to represent the data having higher dimensions. In general, we deal with high dimensional data sets where dimensions refer to different features present in the data set.",understand tensor
107, What do you understand by Boltzmann Machine?,"A Boltzmann machine (also known as stochastic Hopfield network with hidden units) is a type of recurrent neural network. In a Boltzmann machine, nodes make binary decisions with some bias. Boltzmann machines can be strung together to create more sophisticated systems such as deep belief networks. Boltzmann Machines can be used to optimize the solution to a problem. Some important points about Boltzmann Machine- It uses a recurrent structure. It consists of stochastic neurons, which include one of the two possible states, either 1 or 0. The neurons present in this are either in an adaptive state (free state) or clamped state (frozen state). If we apply simulated annealing or discrete Hopfield network, then it would become a Boltzmann Machine. ",understand boltzmann machine
108, What is Model Capacity?,"The capacity of a deep learning neural network controls the scope of the types of mapping functions that it can learn. Model capacity can approximate any given function. When there is a higher model capacity, it means that the larger amount of information can be stored in the network.",model capacity
109, What is the cost function?,"A cost function describes us how well the neural network is performing with respect to its given training sample and the expected output. It may depend on variables such as weights and biases.It provides the performance of a neural network as a whole. In deep learning, our priority is to minimize the cost function. That's why we prefer to use the concept of gradient descent.",cost function
110, Explain gradient descent?,"An optimization algorithm that is used to minimize some function by repeatedly moving in the direction of steepest descent as specified by the negative of the gradient is known as gradient descent. It's an iteration algorithm, in every iteration algorithm, we compute the gradient of a cost function, concerning each parameter and update the parameter of the function via the following formula:Where,ÃÂ - is the parameter vector, ÃÂ± - learning rate,  J(ÃÂ) - is a cost functionIn machine learning, it is used to update the parameters of our model. Parameters represent the coefficients in linear regression and weights in neural networks.",gradient descent
111," Explain the following variant of Gradient Descent: Stochastic, Batch, and Mini-batch?"," Stochastic Gradient Descent Stochastic gradient descent is used to calculate the gradient and update the parameters by using only a single training example. Batch Gradient Descent Batch gradient descent is used to calculate the gradients for the whole dataset and perform just one update at each iteration. Mini-batch Gradient Descent Mini-batch gradient descent is a variation of stochastic gradient descent. Instead of a single training example, mini-batch of samples is used. Mini-batch gradient descent is one of the most popular optimization algorithms. ",following variant gradient descent stochastic batch minibatch
112, What are the main benefits of Mini-batch Gradient Descent?," It is computationally efficient compared to stochastic gradient descent. It improves generalization by finding flat minima. It improves convergence by using mini-batches. We can approximate the gradient of the entire training set, which might help to avoid local minima. ",main benefit minibatch gradient descent
113, What is matrix element-wise multiplication? Explain with an example.,Element-wise matrix multiplication is used to take two matrices of the same dimensions. It further produces another combined matrix with the elements that are a product of corresponding elements of matrix a and b.,matrix elementwise multiplication example
114, What do you understand by a convolutional neural network?,"A convolutional neural network, often called CNN, is a feedforward neural network. It uses convolution in at least one of its layers. The convolutional layer contains a set of filter (kernels). This filter is sliding across the entire input image, computing the dot product between the weights of the filter and the input image. As a result of training, the network automatically learns filters that can detect specific features.",understand convolutional neural network
115, Explain the different layers of CNN.,"There are four layered concepts that we should understand in CNN (Convolutional Neural Network): Convolution This layer comprises of a set of independent filters. All these filters are initialized randomly. These filters then become our parameters which will be learned by the network subsequently. ReLU The ReLu layer is used with the convolutional layer. Pooling It reduces the spatial size of the representation to lower the number of parameters and computation in the network. This layer operates on each feature map independently. Full Collectedness Neurons in a completely connected layer have complete connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can be easily computed with a matrix multiplication followed by a bias offset. ",different layer cnn
116, What is an RNN?,"RNN stands for Recurrent Neural Networks. These are the artificial neural networks which are designed to recognize patterns in sequences of data such as handwriting, text, the spoken word, genomes, and numerical time series data. RNN use backpropagation algorithm for training because of their internal memory. RNN can remember important things about the input they received, which enables them to be very precise in predicting what's coming next.",rnn
117, What are the issues faced while training in Recurrent Networks?,"Recurrent Neural Network uses backpropagation algorithm for training, but it is applied on every timestamp. It is usually known as Back-propagation Through Time (BTT).There are two significant issues with Back-propagation, such as: Vanishing Gradient When we perform Back-propagation, the gradients tend to get smaller and smaller because we keep on moving backward in the Network. As a result, the neurons in the earlier layer learn very slowly if we compare it with the neurons in the later layers.Earlier layers are more valuable because they are responsible for learning and detecting simple patterns. They are the building blocks of the network. If they provide improper or inaccurate results, then how can we expect the next layers and complete network to perform nicely and provide accurate results. The training procedure tales long, and the prediction accuracy of the model decreases. Exploding Gradient Exploding gradients are the main problem when large error gradients accumulate. They provide result in very large updates to neural network model weights during training. Gradient Descent process works best when updates are small and controlled. When the magnitudes of the gradient accumulate, an unstable network is likely to occur. It can cause poor prediction of results or even a model that reports nothing useful. ",issue faced training recurrent network
118, Explain the importance of LSTM.,"LSTM stands for Long short-term memory. It is an artificial RNN (Recurrent Neural Network) architecture, which is used in the field of deep learning. LSTM has feedback connections which makes it a ""general purpose computer."" It can process not only a single data point but also entire sequences of data.They are a special kind of RNN which are capable of learning long-term dependencies.",importance lstm
119, What are the different layers of Autoencoders? Explain briefly.,An autoencoder contains three layers: Encoder The encoder is used to compress the input into a latent space representation. It encodes the input images as a compressed representation in a reduced dimension. The compressed images are the distorted version of the original image. Code The code layer is used to represent the compressed input which is fed to the decoder. Decoder The decoder layer decodes the encoded image back to its original dimension. The decoded image is a reduced reconstruction of the original image. It is automatically reconstructed from the latent space representation. ,different layer autoencoders briefly
120, What do you understand by Deep Autoencoders?,"Deep Autoencoder is the extension of the simple Autoencoder. The first layer present in DeepAutoencoder is responsible for first-order functions in the raw input. The second layer is responsible for second-order functions corresponding to patterns in the appearance of first-order functions. Deeper layers which are available in the Deep Autoencoder tend to learn even high-order features.A deep autoencoder is the combination of two, symmetrical deep-belief networks: First four or five shallow layers represent the encoding half. The other combination of four or five layers makes up the decoding half. ",understand deep autoencoders
121, What are the three steps to developing the necessary assumption structure in Deep learning?,"The procedure of developing an assumption structure involves three specific actions.  The first step contains algorithm development. This particular process is lengthy. The second step contains algorithm analyzing, which represents the in-process methodology.  The third step is about implementing the general algorithm in the final procedure. The entire framework is interlinked and required for throughout the process. ",three step developing necessary assumption structure deep learning
122," What do you understand by Perceptron? Also, explain its type.",A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features. It is an algorithm for supervised learning of binary classifiers. This algorithm is used to enable neurons to learn and processes elements in the training set one at a time.There are two types of perceptrons: Single-Layer Perceptron Single layer perceptrons can learn only linearly separable patterns. Multilayer Perceptrons Multilayer perceptrons or feedforward neural networks with two or more layers have the higher processing power. ÃÂ© Copyright 2011-2021 www.javatpoint.com. All rights reserved. Developed by JavaTpoint.,understand perceptron also type
123,  What is Deep Learning?,"If you are going for a deep learning interview, you definitely know what exactly deep learning is. However, with this question the interviewee expects you to give an in-detail answer, with an example.ÃÂ Deep Learning involves taking large volumes of structured or unstructured data and using complex algorithms to train neural networks. It performs complex operations to extract hidden patterns and features (for instance, distinguishing the image of a cat from that of a dog).",deep learning
124,  What is a Neural Network?,"Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.The most common Neural Networks consist of three network layers: An input layer A hidden layer (this is the most important layer where feature extraction takes place, and adjustments are made to train faster and function better) An output layer Each sheet contains neurons called Ã¢ÂÂnodes,Ã¢ÂÂ performing various operations. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.Post Graduate Program in AI and Machine LearningIn Partnership with Purdue UniversityExplore Course",neural network
125,  What Is a Multi-layer Perceptron(MLP)?,"As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the sameÃÂ structure as a single layer perceptron with one or more hidden layers. A single layer perceptron can classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes.Except for the input layer, each node in the other layers uses a nonlinear activation function. This means the input layers, the data coming in, and the activation function is based upon all nodes and weights being added together, producing the output. MLP uses a supervised learning method called Ã¢ÂÂbackpropagation.Ã¢ÂÂ In backpropagation, the neural network calculates the error with the help of cost function. It propagates this error backward from where it came (adjusts the weights to train the model more accurately).",multilayer perceptronmlp
126,"  What Is Data Normalization, and Why Do We Need It?","The process of standardizing and reforming data is called Ã¢ÂÂData Normalization.Ã¢ÂÂ ItÃ¢ÂÂs a pre-processing step to eliminate data redundancy. Often, data comes in, and you get the same information in different formats. In these cases, you should rescale values to fit into a particular range, achieving better convergence.",data normalization need
127,  What is the Boltzmann Machine?,"One of the most basic Deep Learning models is a Boltzmann Machine, resembling a simplified version of the Multi-Layer Perceptron. This model features a visible input layer and a hidden layer -- just a two-layer neural net that makes stochastic decisions as to whether a neuron should be on or off. Nodes are connected across layers, but no two nodes of the same layer are connected.",boltzmann machine
128,  What Is the Role of Activation Functions in a Neural Network?,"At the most basic level, an activation function decides whether a neuron should be fired or not. It accepts the weighted sum of the inputs and bias as input to any activation function. Step function, Sigmoid, ReLU, Tanh, and Softmax are examples of activation functions.",role activation function neural network
129,  What Is the Cost Function?,"Also referred to as Ã¢ÂÂlossÃ¢ÂÂ or Ã¢ÂÂerror,Ã¢ÂÂ cost function is a measure to evaluate how good your modelÃ¢ÂÂs performance is. ItÃ¢ÂÂs used to compute the error of the output layer during backpropagation. We push that error backward through the neural network and use that during the different training functions.",cost function
130,  What Is Gradient Descent?,Gradient Descent is an optimal algorithm to minimize the cost function or to minimize an error. The aim is to find the local-global minima of a function. This determines the direction the model should take to reduce the error.,gradient descent
131,  What Do You Understand by Backpropagation?,This is one of the most frequently asked deep learning interview questions. Backpropagation is a technique to improve the performance of the network. It backpropagates the error and updates the weights to reduce the error.,understand backpropagation
132,  What Is the Difference Between a Feedforward Neural Network and Recurrent Neural Network?,"In this deep learning interview question, the interviewee expects you to give a detailed answer.A Feedforward Neural Network signals travel in one direction from input to output. There are no feedback loops; the network considers only the current input. It cannot memorize previous inputs (e.g., CNN).Deep Learning Course (with TensorFlow & Keras)Master the Deep Learning Concepts and ModelsView CourseA Recurrent Neural NetworkÃ¢ÂÂs signals travel in both directions, creating a looped network. It considers the current input with the previously received inputs for generating the output of a layer and can memorize past data due to its internal memory.",difference feedforward neural network recurrent neural network
133,  What Are the Applications of a Recurrent Neural Network (RNN)?,"The RNN can be used for sentiment analysis, text mining, and image captioning. Recurrent Neural Networks can also address time series problems such as predicting the prices of stocks in a month or quarter.",application recurrent neural network rnn
134,  What Are the Softmax and ReLU Functions?,"Softmax is an activation function that generates the output between zero and one. It divides each output, such that the total sum of the outputs is equal to one. Softmax is often used for output layers.ReLU (or Rectified Linear Unit) is the most widely used activation function. It gives an output of X if X is positive and zeros otherwise. ReLU is often used for hidden layers.",softmax relu function
135,  What Are Hyperparameters?,"This is another frequently asked deep learning interview question. With neural networks, youÃ¢ÂÂre usually working with hyperparameters once the data is formatted correctly. A hyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, etc.).",hyperparameters
136,  What Will Happen If the Learning Rate Is Set Too Low or Too High?,"When your learning rate is too low, training of the model will progress very slowly as we are making minimal updates to the weights. It will take many updates before reaching the minimum point.If the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights. It may fail to converge (model can give a good output) or even diverge (data is too chaotic for the network to train).",happen learning rate set low high
137,  What Is Dropout and Batch Normalization?,Dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfitting of data (typically dropping 20 percent of the nodes). It doubles the number of iterations needed to converge the network.Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.,dropout batch normalization
138,  What Is the Difference Between Batch Gradient Descent and Stochastic Gradient Descent?,"Batch Gradient DescentStochastic Gradient DescentThe batch gradient computes the gradient using the entire dataset.It takes time to converge because the volume of data is huge, and weights update slowly.The stochastic gradient computes the gradient using a single sample.It converges much faster than the batch gradient because it updates weight more frequently.",difference batch gradient descent stochastic gradient descent
139,"  What is Overfitting and Underfitting, and How to Combat Them?","Overfitting occurs when the model learns the details and noise in the training data to the degree that it adversely impacts the execution of the model on new information. It is more likely to occur with nonlinear models that have more flexibility when learning a target function. An example would be if a model is looking at cars and trucks, but only recognizes trucks that have a specific box shape. It might not be able to notice a flatbed truck because there's only a particular kind of truck it saw in training. The model performs well on training data, but not in the real world.Underfitting alludes to a model that is neither well-trained on data nor can generalize to new information. This usually happens when there is less and incorrect data to train a model. Underfitting has both poor performance and accuracy.To combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-fold cross-validation) and by having a validation dataset to evaluate the model.",overfitting underfitting combat
140,  How Are Weights Initialized in a Network?,"There are two methods here: we can either initialize the weights to zero or assign them randomly.Initializing all weights to 0: This makes your model similar to a linear model. All the neurons and every layer perform the same operation, giving the same output and making the deep net useless.Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to 0. It gives better accuracy to the model since every neuron performs different computations. This is the most commonly used method.Free Deep Learning for Beginners CourseMaster the Basics of Deep LearningEnroll Now",weight initialized network
141,  What Are the Different Layers on CNN?,"There are four layers in CNN: Convolutional Layer - ÃÂ the layer that performs a convolutional operation, creating several smaller picture windows to go over the data. ReLU Layer - it brings non-linearity to the network and converts all the negative pixels to zero. The output is a rectified feature map. Pooling Layer - pooling is a down-sampling operation that reduces the dimensionality of the feature map. Fully Connected Layer - this layer recognizes and classifies the objects in the image. ",different layer cnn
142,"  What is Pooling on CNN, and How Does It Work?",Pooling is used to reduce the spatial dimensions of a CNN. It performs down-sampling operations to reduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix.,pooling cnn work
143,  How Does an LSTM Network Work?,"Long-Short-Term Memory (LSTM) is a special kind of recurrent neural network capable of learning long-term dependencies, remembering information for long periods as its default behavior. There are three steps in an LSTM network:",lstm network work
144,  What Are Vanishing and Exploding Gradients?,"While training an RNN, your slope can become either too small or too large; this makes the training difficult. When the slope is too small, the problem is known as a Ã¢ÂÂVanishing Gradient.Ã¢ÂÂ When the slope tends to grow exponentially instead of decaying, itÃ¢ÂÂs referred to as an Ã¢ÂÂExploding Gradient.Ã¢ÂÂ Gradient problems lead to long training times, poor performance, and low accuracy.",vanishing exploding gradient
145,  Why is Tensorflow the Most Preferred Library in Deep Learning?,"Tensorflow provides both C++ and Python APIs, making it easier to work on and has a faster compilation time compared to other Deep Learning libraries like Keras and Torch. Tensorflow supports both CPU and GPU computing devices.",tensorflow preferred library deep learning
146,  What Do You Mean by Tensor in Tensorflow?,This is another most frequently asked deep learning interview question. A tensor is a mathematical object represented as arrays of higher dimensions. These arrays of data with different dimensions and ranks fed as input to the neural network are called Ã¢ÂÂTensors.Ã¢ÂÂ,mean tensor tensorflow
147,  What Are the Programming Elements in Tensorflow?,"Constants - Constants are parameters whose value does not change. To define a constant we use ÃÂ tf.constant() command. For example:a = tf.constant(2.0,tf.float32)b = tf.constant(3.0)Print(a, b)Variables - Variables allow us to add new trainable parameters to graph. To define a variable, we use the tf.Variable() command and initialize them before running the graph in a session. An example:W = tf.Variable([.3].dtype=tf.float32)b = tf.Variable([-.3].dtype=tf.float32)Placeholders - these allow us to feed data to a tensorflow model from outside a model. It permits a value to be assigned later. To define a placeholder, we use the tf.placeholder() command. An example:a = tf.placeholder (tf.float32)b = a*2with tf.Session() as sess:result = sess.run(b,feed_dict={a:3.0})print resultSessions - a session is run to evaluate the nodes. This is called the Ã¢ÂÂTensorflow runtime.Ã¢ÂÂ For example:a = tf.constant(2.0)b = tf.constant(4.0)c = a+b# Launch SessionSess = tf.Session()# Evaluate the tensor cprint(sess.run(c))FREE Machine Learning CourseLearn In-demand Machine Learning Skills and ToolsStart Learning",programming element tensorflow
148,  Explain a Computational Graph.,"Everything in a tensorflow is based on creating a computational graph. It has a network of nodes where each node operates, Nodes represent mathematical operations, and edges represent tensors. Since data flows in the form of a graph, it is also called a Ã¢ÂÂDataFlow Graph.Ã¢ÂÂ",computational graph
149,  Explain Generative Adversarial Network.,"Suppose there is a wine shop purchasing wine from dealers, which they resell later. But some dealers sell fake wine. In this case, the shop owner should be able to distinguish between fake and authentic wine.The forger will try different techniques to sell fake wine and make sure specific techniques go past the shop ownerÃ¢ÂÂs check. The shop owner would probably get some feedback from wine experts that some of the wine is not original. The owner would have to improve how he determines whether a wine is fake or authentic.The forgerÃ¢ÂÂs goal is to create wines that are indistinguishable from the authentic ones while the shop owner intends to tell if the wine is real or not accurately.Let us understand this example with the help of an image shown above.There is a noise vector coming into the forger who is generating fake wine.Here the forger acts as a Generator.The shop owner acts as a Discriminator.The Discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. The shop owner has to figure out whether it is real or fake.So, there are two primary components of Generative Adversarial Network (GAN) named: Generator Discriminator The generator is a CNN that keeps keys producing images and is closer in appearance to the real images while the discriminator tries to determine the difference between real and fake images The ultimate aim is to make the discriminator learn to identify real and fake images.",generative adversarial network
150,  What Is an Auto-encoder?,This Neural Network has three layers in which the input neurons are equal to the output neurons. The network's target outside is the same as the input. It uses dimensionality reduction to restructure the input. It works by compressing the image input to a latent space representation then reconstructing the output from this representation.,autoencoder
151,  What Is Bagging and Boosting?,"Bagging and Boosting are ensemble techniques to train multiple models using the same learning algorithm and then taking a call.With Bagging, we take a dataset and split it into training data and test data. Then we randomly select data to place into the bags and train the model separately.With Boosting, the emphasis is on selecting data points which give wrong output to improve the accuracy.",bagging boosting
152,  What is the difference between Machine Learning and Deep Learning?,"Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby helping them improve with experience. Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby forming neural networks.  Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby helping them improve with experience.Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby forming neural networks.",difference machine learning deep learning
153,  What is a perceptron?,"A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output. A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.  A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output.A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.",perceptron
154,  How is Deep Learning better than Machine Learning?,"Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.  Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.",deep learning better machine learning
155,  What are some of the most used applications of Deep Learning?,Deep Learning is used in a variety of fields today. The most used ones are as follows:  Sentiment Analysis Computer Vision Automatic Text Generation Object Detection Natural Language Processing Image Recognition   Deep Learning is used in a variety of fields today. The most used ones are as follows: Sentiment Analysis Computer Vision Automatic Text Generation Object Detection Natural Language Processing Image Recognition ,used application deep learning
156,  What is the meaning of overfitting?,"Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information. This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.  Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information. This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.",meaning overfitting
157,  What are activation functions?,Activation functions are entities in Deep Learning that are used to translate inputs into a usable output parameter. It is a function that decides if a neuron needs activation or not by calculating the weighted sum on it with the bias. Using an activation function makes the model output to be non-linear. There are many types of activation functions:  ReLU Softmax Sigmoid Linear Tanh  Get 50% Hike!Master Most in Demand Skills Now !                                             Activation functions are entities in Deep Learning that are used to translate inputs into a usable output parameter. It is a function that decides if a neuron needs activation or not by calculating the weighted sum on it with the bias.Using an activation function makes the model output to be non-linear. There are many types of activation functions: ReLU Softmax Sigmoid Linear Tanh Get 50% Hike!Master Most in Demand Skills Now !,activation function
158,  Why is Fourier transform used in Deep Learning?,Fourier transform is an effective package used for analyzing and managing large amounts of data present in a database. It can take in real-time array data and process it quickly. This ensures that high efficiency is maintained and also makes the model more open to processing a variety of signals.  Fourier transform is an effective package used for analyzing and managing large amounts of data present in a database. It can take in real-time array data and process it quickly. This ensures that high efficiency is maintained and also makes the model more open to processing a variety of signals.,fourier transform used deep learning
159,  What are the steps involved in training a perception in Deep Learning?,There are five main steps that determine the learning of a perceptron:  Initialize thresholds and weights Provide inputs Calculate outputs Update weights in each step Repeat steps 2 to 4   There are five main steps that determine the learning of a perceptron:,step involved training perception deep learning
160,  What is the use of the loss function?,"The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset. The loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training.  The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset. The loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training.",use loss function
161,  What are some of the Deep Learning frameworks or tools that you have used?,"This question is quite common in a Deep Learning interview. Make sure to answer based on the experience you have with the tools. However, some of the top Deep Learning frameworks out there today are:  TensorFlow Keras PyTorch Caffe2 CNTK MXNet Theano  This question is quite common in a Deep Learning interview. Make sure to answer based on the experience you have with the tools.However, some of the top Deep Learning frameworks out there today are: TensorFlow Keras PyTorch Caffe2 CNTK MXNet Theano ",deep learning framework tool used
162,  What is the use of the swish function?,The swish function is a self-gated activation function developed by Google. It is now a popular activation function used by many as Google claims that it outperforms all of the other activation functions in terms of computational efficiency. The swish function is a self-gated activation function developed by Google. It is now a popular activation function used by many as Google claims that it outperforms all of the other activation functions in terms of computational efficiency.,use swish function
163,  What are autoencoders?,"Autoencoders are artificial neural networks that learn without any supervision. Here, these networks have the ability to automatically learn by mapping the inputs to the corresponding outputs. Autoencoders, as the name suggests, consist of two entities:  Encoder: Used to fit the input into an internal computation state Decoder: Used to convert the computational state back into the output  Autoencoders are artificial neural networks that learn without any supervision. Here, these networks have the ability to automatically learn by mapping the inputs to the corresponding outputs.Autoencoders, as the name suggests, consist of two entities: Encoder: Used to fit the input into an internal computation state Decoder: Used to convert the computational state back into the output ",autoencoders
164,  What are the steps to be followed to use the gradient descent algorithm?,There are five main steps that are used to initialize and use the gradient descent algorithm:  Initialize biases and weights for the network Send input data through the network (the input layer) Calculate the difference (the error) between expected and predicted values Change values in neurons to minimize the loss function Multiple iterations to determine the best weights for efficient working  There are five main steps that are used to initialize and use the gradient descent algorithm: Initialize biases and weights for the network Send input data through the network (the input layer) Calculate the difference (the error) between expected and predicted values Change values in neurons to minimize the loss function Multiple iterations to determine the best weights for efficient working ,step followed use gradient descent algorithm
165,  Differentiate between a single-layer perceptron and a multi-layer perceptron.,    Single-layer Perceptron Multi-layer Perceptron   Cannot classify non-linear data points Can classify non-linear data   Takes in a limited amount of parameters Withstands a lot of parameters   Less efficient with large data Highly efficient with large datasets      Career Transition                                                   Career Transition,differentiate singlelayer perceptron multilayer perceptron
166,  What is data normalization in Deep Learning?,Data normalization is a preprocessing step that is used to refit the data into a specific range. This ensures that the network can learn effectively as it has better convergence when performing backpropagation. Data normalization is a preprocessing step that is used to refit the data into a specific range. This ensures that the network can learn effectively as it has better convergence when performing backpropagation.,data normalization deep learning
167,  What is forward propagation?,"Forward propagation is the scenario where inputs are passed to the hidden layer with weights. In every single hidden layer, the output of the activation function is calculated until the next layer can be processed. It is called forward propagation as the process begins from the input layer and moves toward the final output layer. Forward propagation is the scenario where inputs are passed to the hidden layer with weights. In every single hidden layer, the output of the activation function is calculated until the next layer can be processed. It is called forward propagation as the process begins from the input layer and moves toward the final output layer.",forward propagation
168,  What is backpropagation?,"Backprobation is used to minimize the cost function by first seeing how the value changes when weights and biases are tweaked in the neural network. This change is easily calculated by understanding the gradient at every hidden layer. It is called backpropagation as the process begins from the output layer, moving backward to the input layers. Backprobation is used to minimize the cost function by first seeing how the value changes when weights and biases are tweaked in the neural network. This change is easily calculated by understanding the gradient at every hidden layer. It is called backpropagation as the process begins from the output layer, moving backward to the input layers.",backpropagation
169,  What are hyperparameters in Deep Learning?,"Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers and more, present in the neural network. Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers and more, present in the neural network.",hyperparameters deep learning
170,  How can hyperparameters be trained in neural networks?,"Hyperparameters can be trained using four components as shown below:  Batch size: This is used to denote the size of the input chunk. Batch sizes can be varied and cut into sub-batches based on the requirement. Epochs: An epoch denotes the number of times the training data is visible to the neural network so that it can train. Since the process is iterative, the number of epochs will vary based on the data. Momentum: Momentum is used to understand the next consecutive steps that occur with the current data being executed at hand. It is used to avoid oscillations when training. Learning rate: Learning rate is used as a parameter to denote the time required for the network to update the parameters and learn.  Next up on this top Deep Learning interview questions and answers blog, let us take a look at the intermediate questions.  Intermediate Interview Questions Hyperparameters can be trained using four components as shown below: Batch size: This is used to denote the size of the input chunk. Batch sizes can be varied and cut into sub-batches based on the requirement. Epochs: An epoch denotes the number of times the training data is visible to the neural network so that it can train. Since the process is iterative, the number of epochs will vary based on the data. Momentum: Momentum is used to understand the next consecutive steps that occur with the current data being executed at hand. It is used to avoid oscillations when training. Learning rate: Learning rate is used as a parameter to denote the time required for the network to update the parameters and learn. Next up on this top Deep Learning interview questions and answers blog, let us take a look at the intermediate questions.",hyperparameters trained neural network
171,  What is the meaning of dropout in Deep Learning?,"Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby causing lower efficiency. Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby causing lower efficiency.",meaning dropout deep learning
172,  What are tensors?,"Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors are easily understood and broadly used. Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors are easily understood and broadly used.",tensor
173,  What is the meaning of model capacity in Deep Learning?,"In Deep Learning, model capacity refers to the capacity of the model to take in a variety of mapping functions. Higher model capacity means a large amount of information can be stored in the network. We will check out neural network interview questions alongside as it is also a vital part of Deep Learning. In Deep Learning, model capacity refers to the capacity of the model to take in a variety of mapping functions. Higher model capacity means a large amount of information can be stored in the network.We will check out neural network interview questions alongside as it is also a vital part of Deep Learning.",meaning model capacity deep learning
174,  What is a Boltzmann machine?,"A Boltzmann machine is a type of recurrent neural network that uses binary decisions, alongside biases, to function. These neural networks can be hooked up together to create deep belief networks, which are very sophisticated and used to solve the most complex problems out there. A Boltzmann machine is a type of recurrent neural network that uses binary decisions, alongside biases, to function. These neural networks can be hooked up together to create deep belief networks, which are very sophisticated and used to solve the most complex problems out there.",boltzmann machine
175,  What are some of the advantages of using TensorFlow?,"TensorFlow has numerous advantages, and some of them are as follows:  High amount of flexibility and platform independence Trains using CPU and GPU Supports auto differentiation and its features Handles threads and asynchronous computation easily Open-source Has a large community   Courses you may like      TensorFlow has numerous advantages, and some of them are as follows: High amount of flexibility and platform independence Trains using CPU and GPU Supports auto differentiation and its features Handles threads and asynchronous computation easily Open-source Has a large community Courses you may like",advantage using tensorflow
176,  What is a computational graph in Deep Learning?,"A computation graph is a series of operations that are performed to take in inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability. If you are looking forward to becoming an expert in Deep Learning, make sure to check out IntellipaatÃ¢ÂÂs AI Engineer Course. A computation graph is a series of operations that are performed to take in inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability.If you are looking forward to becoming an expert in Deep Learning, make sure to check out IntellipaatÃ¢ÂÂs AI Engineer Course.",computational graph deep learning
177,  What is a CNN?,CNNs are convolutional neural networks that are used to perform analysis on images and visuals. These classes of neural networks can input a multi-channel image and work on it easily. These Deep Learning questions must be answered in a concise way. So make sure to understand them and revisit them if necessary. CNNs are convolutional neural networks that are used to perform analysis on images and visuals. These classes of neural networks can input a multi-channel image and work on it easily.These Deep Learning questions must be answered in a concise way. So make sure to understand them and revisit them if necessary.,cnn
178,  What are the various layers present in a CNN?,There are four main layers that form a convolutional neural network:  Convolution: These are layers consisting of entities called filters that are used as parameters to train the network. ReLu: It is used as the activation function and used always with the convolution layer. Pooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage. Connectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily.  There are four main layers that form a convolutional neural network: Convolution: These are layers consisting of entities called filters that are used as parameters to train the network. ReLu: It is used as the activation function and used always with the convolution layer. Pooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage. Connectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily. ,various layer present cnn
179,  What is an RNN in Deep Learning?,"RNNs stand for recurrent neural networks, which form to be a popular type of artificial neural network. They are used to process sequences of data, text, genomes, handwriting, and more. RNNs make use of backpropagation for the training requirements. RNNs stand for recurrent neural networks, which form to be a popular type of artificial neural network. They are used to process sequences of data, text, genomes, handwriting, and more. RNNs make use of backpropagation for the training requirements.",rnn deep learning
180,  What is a vanishing gradient when using RNNs?,"Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby causing efficiency problems in the network. Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby causing efficiency problems in the network.",vanishing gradient using rnns
181,  What is exploding gradient descent in Deep Learning?,Exploding gradients are an issue causing a scenario that clumps up the gradients. This creates a large number of updates of the weights in the model when training. The working of gradient descent is based on the condition that the updates are small and controlled. Controlling the updates will directly affect the efficiency of the model. Exploding gradients are an issue causing a scenario that clumps up the gradients. This creates a large number of updates of the weights in the model when training.The working of gradient descent is based on the condition that the updates are small and controlled. Controlling the updates will directly affect the efficiency of the model.,exploding gradient descent deep learning
182,  What is the use of LSTM?,LSTM stands for long short-term memory. It is a type of RNN that is used to sequence a string of data. It consists of feedback chains that give it the ability to perform like a general-purpose computational entity. LSTM stands for long short-term memory. It is a type of RNN that is used to sequence a string of data. It consists of feedback chains that give it the ability to perform like a general-purpose computational entity.,use lstm
183,  Where are autoencoders used?,Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones:  Adding color to blackÃ¢ÂÂwhite images Removing noise from images Dimensionality reduction Feature removal and variation  Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones: Adding color to blackÃ¢ÂÂwhite images Removing noise from images Dimensionality reduction Feature removal and variation ,autoencoders used
184,  What are the types of autoencoders?,There are four main types of autoencoders:  Deep autoencoders Convolutional autoencoders Sparse autoencoders Contractive autoencoders  There are four main types of autoencoders: Deep autoencoders Convolutional autoencoders Sparse autoencoders Contractive autoencoders ,type autoencoders
185,  What is a Restricted Boltzmann Machine?,"A Restricted Boltzmann Machine, or RBM for short, is an undirected graphical model that is popularly used in Deep Learning today. It is an algorithm that is used to perform:  Dimensionality reduction Regression Classification Collaborative filtering Topic modeling  Next up on this top Deep Learning interview questions and answers blog, let us take a look at the advanced questions.   Advanced Interview Questions A Restricted Boltzmann Machine, or RBM for short, is an undirected graphical model that is popularly used in Deep Learning today. It is an algorithm that is used to perform: Dimensionality reduction Regression Classification Collaborative filtering Topic modeling Next up on this top Deep Learning interview questions and answers blog, let us take a look at the advanced questions.",restricted boltzmann machine
186,  What are some of the limitations of Deep Learning?,There are a few disadvantages of Deep Learning as mentioned below:  Networks in Deep Learning require a huge amount of data to train well. Deep Learning concepts can be complex to implement sometimes. Achieving a high amount of model efficiency is difficult in many cases.  These are some of the vital advanced deep learning interview questions that you have to know about! There are a few disadvantages of Deep Learning as mentioned below: Networks in Deep Learning require a huge amount of data to train well. Deep Learning concepts can be complex to implement sometimes. Achieving a high amount of model efficiency is difficult in many cases. These are some of the vital advanced deep learning interview questions that you have to know about!,limitation deep learning
187,  What are the variants of gradient descent?,"There are three variants of gradient descent as shown below:  Stochastic gradient descent: A single training example is used for the calculation of gradient and for updating parameters. Batch gradient descent: Gradient is calculated for the entire dataset, and parameters are updated at every iteration. Mini-batch gradient descent: Samples are broken down into smaller-sized batches and then worked on as in the case of stochastic gradient descent.  There are three variants of gradient descent as shown below: Stochastic gradient descent: A single training example is used for the calculation of gradient and for updating parameters. Batch gradient descent: Gradient is calculated for the entire dataset, and parameters are updated at every iteration. Mini-batch gradient descent: Samples are broken down into smaller-sized batches and then worked on as in the case of stochastic gradient descent. ",variant gradient descent
188,  Why is mini-batch gradient descent so popular?,Mini-batch gradient descent is popular as:  It is more efficient when compared to stochastic gradient descent. Generalization is done by finding the flat minima. It helps avoid the local minima by allowing the approximation of the gradient for the entire dataset.  Mini-batch gradient descent is popular as: It is more efficient when compared to stochastic gradient descent. Generalization is done by finding the flat minima. It helps avoid the local minima by allowing the approximation of the gradient for the entire dataset. ,minibatch gradient descent popular
189,  What are deep autoencoders?,"Deep autoencoders are an extension of the regular autoencoders. Here, the first layer is responsible for the first-order function execution of the input. The second layer will take care of the second-order functions, and it goes on. Usually, a deep autoencoder is a combination of two or more symmetrical deep-belief networks where:  The first five shallow layers consist of the encoding part The other layers take care of the decoding part  On the next set of Deep Learning questions, let us look further into the topic. Deep autoencoders are an extension of the regular autoencoders. Here, the first layer is responsible for the first-order function execution of the input. The second layer will take care of the second-order functions, and it goes on.Usually, a deep autoencoder is a combination of two or more symmetrical deep-belief networks where: The first five shallow layers consist of the encoding part The other layers take care of the decoding part On the next set of Deep Learning questions, let us look further into the topic.",deep autoencoders
190,  Why is the Leaky ReLU function used in Deep Learning?,"Leaky ReLU, also called LReL, is used to manage a function to allow the passing of small-sized negative values if the input value to the network is less than zero. Leaky ReLU, also called LReL, is used to manage a function to allow the passing of small-sized negative values if the input value to the network is less than zero.",leaky relu function used deep learning
191,  What are some of the examples of supervised learning algorithms in Deep Learning?,There are three main supervised learning algorithms in Deep Learning:  Artificial neural networks Convolutional neural networks Recurrent neural networks  There are three main supervised learning algorithms in Deep Learning: Artificial neural networks Convolutional neural networks Recurrent neural networks ,example supervised learning algorithm deep learning
192,  What are some of the examples of unsupervised learning algorithms in Deep Learning?,"There are three main unsupervised learning algorithms in Deep Learning:  Autoencoders Boltzmann machines Self-organizing maps  Next up, let us look atÃÂ  more neural network interview questions that will help you ace the interviews. There are three main unsupervised learning algorithms in Deep Learning: Autoencoders Boltzmann machines Self-organizing maps Next up, let us look atÃÂ  more neural network interview questions that will help you ace the interviews.",example unsupervised learning algorithm deep learning
193,  Can we initialize the weights of a network to start from zero?,"Yes, it is possible to begin with zero initialization. However, it is not recommended to use because setting up the weights to zero initially will cause all of the neurons to produce the same output and the same gradients when performing backpropagation. This means that the network will not have the ability to learn at all due to the absence of asymmetry between each of the neurons. Yes, it is possible to begin with zero initialization. However, it is not recommended to use because setting up the weights to zero initially will cause all of the neurons to produce the same output and the same gradients when performing backpropagation. This means that the network will not have the ability to learn at all due to the absence of asymmetry between each of the neurons.",initialize weight network start zero
194,  What is the meaning of valid padding and same padding in CNN?," Valid padding: It is used when there is no requirement for padding. The output matrix will have the dimensions (n Ã¢ÂÂ f + 1) X (n Ã¢ÂÂ f + 1) after convolution. Same padding: Here, padding elements are added all around the output matrix. It will have the same dimensions as the input matrix.   Valid padding: It is used when there is no requirement for padding. The output matrix will have the dimensions (n Ã¢ÂÂ f + 1) X (n Ã¢ÂÂ f + 1) after convolution. Same padding: Here, padding elements are added all around the output matrix. It will have the same dimensions as the input matrix. ",meaning valid padding padding cnn
195,  What are some of the applications of transfer learning in Deep Learning?,"Transfer learning is a scenario where a large model is trained on a dataset with a large amount of data and this model is used on simpler datasets, thereby resulting in extremely efficient and accurate neural networks. The popular examples of transfer learning are in the case of:  BERT ResNet GPT-2 VGG-16  Transfer learning is a scenario where a large model is trained on a dataset with a large amount of data and this model is used on simpler datasets, thereby resulting in extremely efficient and accurate neural networks.The popular examples of transfer learning are in the case of: BERT ResNet GPT-2 VGG-16 ",application transfer learning deep learning
196,  How is the transformer architecture better than RNNs in Deep Learning?,"With the use of sequential processing, programmers were up against:  The usage of high processing power The difficulty of parallel execution  This caused the rise of the transformer architecture. Here, there is a mechanism called attention mechanism, which is used to map all of the dependencies between sentences, thereby making huge progress in the case of NLP models.  With the use of sequential processing, programmers were up against: The usage of high processing power The difficulty of parallel execution This caused the rise of the transformer architecture. Here, there is a mechanism called attention mechanism, which is used to map all of the dependencies between sentences, thereby making huge progress in the case of NLP models.",transformer architecture better rnns deep learning
197,  What are the steps involved in the working of an LSTM network?,There are three main steps involved in the working of an LSTM network:  The network picks up the information that it has to remember and identifies what to forget. Cell state values are updated based on Step 1. The network calculates and analyzes which part of the current state should make it to the output.  There are three main steps involved in the working of an LSTM network: The network picks up the information that it has to remember and identifies what to forget. Cell state values are updated based on Step 1. The network calculates and analyzes which part of the current state should make it to the output. ,step involved working lstm network
198,  What are the elements in TensorFlow that are programmable?,"In TensorFlow, users can program three elements:  Constants Variables Placeholders  In TensorFlow, users can program three elements: Constants Variables Placeholders ",element tensorflow programmable
199,  What is the meaning of bagging and boosting in Deep Learning?,Bagging is the concept of splitting a dataset and randomly placing it into bags for training the model. Boosting is the scenario where incorrect data points are used to force the model to produce the wrong output. This is used to retrain the model and increase accuracy. Bagging is the concept of splitting a dataset and randomly placing it into bags for training the model.Boosting is the scenario where incorrect data points are used to force the model to produce the wrong output. This is used to retrain the model and increase accuracy.,meaning bagging boosting deep learning
200,  What are generative adversarial networks (GANs)?,"Generative adversarial networks are used to achieve generative modeling in Deep Learning. It is an unsupervised task that involves the discovery of patterns in the input data to generate the output. The generator is used to generate new examples, while the discriminator is used to classify the examples generated by the generator. Generative adversarial networks are used to achieve generative modeling in Deep Learning. It is an unsupervised task that involves the discovery of patterns in the input data to generate the output.The generator is used to generate new examples, while the discriminator is used to classify the examples generated by the generator.",generative adversarial network gans
201,  Why are generative adversarial networks (GANs) so popular?,"Generative adversarial networks are used for a variety of purposes. In the case of working with images, they have a high amount of traction and efficient working.  Creation of art: GANs are used to create artistic images, sketches, and paintings. Image enhancement: They are used to greatly enhance the resolution of the input images. Image translation: They are also used to change certain aspects, such as day to night and summer to winter, in images easily.  If you are looking forward to becoming an expert in Deep Learning, make sure to check out IntellipaatÃ¢ÂÂs AI Course. With this program, you can become proficient in all of the concepts of Deep Learning and AI and earn a course certificate as well. Generative adversarial networks are used for a variety of purposes. In the case of working with images, they have a high amount of traction and efficient working. Creation of art: GANs are used to create artistic images, sketches, and paintings. Image enhancement: They are used to greatly enhance the resolution of the input images. Image translation: They are also used to change certain aspects, such as day to night and summer to winter, in images easily. ",generative adversarial network gans popular
202,  Why is it necessary to introduce non-linearities in a neural network?,"Solution: otherwise, we would have a composition of linear functions, which is also a linear function, giving a linear model. A linear model has a much smaller number of parameters, and is therefore limited in the complexity it can model.",necessary introduce nonlinearities neural network
203,  Describe two ways of dealing with the vanishing gradient problem in a neural network.,Solution:Using ReLU activation instead of sigmoid.Using Xavier initialization.,two way dealing vanishing gradient problem neural network
204,  What are some advantages in using a CNN (convolutional neural network) rather than a DNN (dense neural network) in an image classification task?,"Solution: while both models can capture the relationship between close pixels, CNNs have the following properties:It is translation invariant Ã¢ÂÂ the exact location of the pixel is irrelevant for the filter.It is less likely to overfit Ã¢ÂÂ the typical number of parameters in a CNN is much smaller than that of a DNN.Gives us a better understanding of the model Ã¢ÂÂ we can look at the filtersÃ¢ÂÂ weights and visualize what the network Ã¢ÂÂlearnedÃ¢ÂÂ.Hierarchical nature Ã¢ÂÂ learns patterns in by describing complex patterns using simpler ones.",advantage using cnn convolutional neural network rather dnn dense neural network classification task
205,  Describe two ways to visualize features of a CNN in an image classification task.,"Solution:Input occlusion Ã¢ÂÂ cover a part of the input image and see which part affect the classification the most. For instance, given a trained image classification model, give the images below as input. If, for instance, we see that the 3rd image is classified with 98% probability as a dog, while the 2nd image only with 65% accuracy, it means that the part covered in the 2nd image is more important.Activation Maximization Ã¢ÂÂ the idea is to create an artificial input image that maximize the target response (gradient ascent).",two way visualize feature cnn classification task
206,"  Is trying the following learning rates: 1, 2,Ã¢ÂÂ¦, 5 a good strategy to optimize the learning rate?","Solution: No, it is recommended to try a logarithmic scale to optimize the learning rate.",trying following learning rate 1 2ãââ 5 good strategy optimize learning rate
207,  Suppose you have a NN with 3 layers and ReLU activations. What will happen if we initialize all the weights with the same value? what if we only had 1 layer (i.e linear/logistic regression?),"Solution: If we initialize all the weights to be the same we would not be able to break the symmetry; i.e, all gradients will be updated the same and the network will not be able to learn. In the 1-layers scenario, however, the cost function is convex (linear/sigmoid) and thus the weights will always converge to the optimal point, regardless of the initial value (convergence may be slower).",suppose nn 3 layer relu activation happen initialize weight value 1 layer ie linearlogistic regression
208,  Explain the idea behind the Adam optimizer.,"Solution: Adam, or adaptive momentum, combines two ideas to improve convergence: per-parameter updates which give faster convergence, and momentum which helps to avoid getting stuck in saddle point.",idea behind adam optimizer
209,"  Compare batch, mini-batch and stochastic gradient descent.","Solution: batch refers to estimating the data by taking the entire data, mini-batch by sampling a few datapoints, and SGD refers to update the gradient one datapoint at each epoch. The tradeoff here is between how precise the calculation of the gradient is versus what size of batch we can keep in memory. Moreover, taking mini-batch rather than the entire batch has a regularizing effect by adding random noise at each epoch.",compare batch minibatch stochastic gradient descent
210,  What is data augmentation? Give examples.,"Solution: Data augmentation is a technique to increase the input data by performing manipulations on the original data. For instance in images, one can: rotate the image, reflect (flip) the image, add Gaussian blur.",data augmentation give example
211,  What is the idea behind GANs?,"Solution: GANs, or generative adversarial networks, consist of two networks (D,G) where D is the Ã¢ÂÂdiscriminatorÃ¢ÂÂ network and G is the Ã¢ÂÂgenerativeÃ¢ÂÂ network. The goal is to create data Ã¢ÂÂ images, for instance, which are undistinguishable from real images. Suppose we want to create an adversarial example of a cat. The network G will generate images. The network D will classify images according to whether they are a cat or not. The cost function of G will be constructed such that it tries to Ã¢ÂÂfoolÃ¢ÂÂ D Ã¢ÂÂ to classify its output always as cat.",idea behind gans
212,  What are the advantages of using Batchnorm?,Solution: Batchnorm accelerates the training process. It also (as a byproduct of including some noise) has a regularizing effect.,advantage using batchnorm
213,  What is multi-task learning? When should it be used?,"Solution: Multi-tasking is useful when we have a small amount of data for some task, and we would benefit from training a model on a large dataset of another task. Parameters of the models are shared Ã¢ÂÂ either in a Ã¢ÂÂhardÃ¢ÂÂ way (i.e the same parameters) or a Ã¢ÂÂsoftÃ¢ÂÂ way (i.e regularization/penalty to the cost function).",multitask learning used
214,  What is end-to-end learning? Give a few of its advantages.,"Solution: End-to-end learning is usually a model which gets the raw data and outputs directly the desired outcome, with no intermediate tasks or feature engineering. It has several advantages, among which: there is no need to handcraft features, and it generally leads to lower bias.",endtoend learning give advantage
215,  What happens if we use a ReLU activation and then a sigmoid as the final layer?,"Solution: Since ReLU always outputs a non-negative result, the network will constantly predict one class for all the inputs!",happens use relu activation sigmoid final layer
216,  How to solve the exploding gradient problem?,"Solution: A simple solution to the exploding gradient problem is gradient clipping Ã¢ÂÂ taking the gradient to be ÃÂ±M when its absolute value is bigger than M, where M is some large number.",solve exploding gradient problem
217,  Is it necessary to shuffle the training data when using batch gradient descent?,"Solution: No, because the gradient is calculated at each epoch using the entire training data, so shuffling does not make a difference.",necessary shuffle training data using batch gradient descent
218,"  When using mini batch gradient descent, why is it important to shuffle the data?","Solution: otherwise, suppose we train a NN classifier and have two classes Ã¢ÂÂ A and B, and that all samples of one class come before the other class. Not shuffling the data will make the weights converge to a wrong value.",using mini batch gradient descent important shuffle data
219,  Describe some hyperparameters for transfer learning.,"Solution: How many layers to keep, how many layers to add, how many to freeze.",hyperparameters transfer learning
220,  Is dropout used on the test set?,Solution: No! only in the train set. Dropout is a regularization technique that is applied in the training process.,dropout used test set
221,  Explain why dropout in a neural network acts as a regularizer.,"Solution: There are several (related) explanations to why dropout works. It can be seen as a form of model averaging Ã¢ÂÂ at each step we Ã¢ÂÂturn offÃ¢ÂÂ a part of the model and average the models we get. It also adds noise, which naturally has a regularizing effect. It also leads to more sparsity of the weights and essentially prevents co-adaptation of neurons in the network.",dropout neural network act regularizer
222,  Give examples in which a many-to-one RNN architecture is appropriate.,"Solution: A few examples are: sentiment analysis, gender recognition from speech.",give example manytoone rnn architecture appropriate
223,  When cant we use BiLSTM? Explain what assumption has to be made.,"Solution: in any bi-directional model, we assume that we have access to the next elements of the sequence in a given Ã¢ÂÂtimeÃ¢ÂÂ. This is the case for text data (i.e sentiment analysis, translation etc.), but not the case for time-series data.",cant use bilstm assumption made
224,  True/false: adding L2 regularization to a RNN can help with the vanishing gradient problem.,"Solution: false! Adding L2 regularization will shrink the weights towards zero, which can actually make the vanishing gradients worse in some cases.",truefalse adding l2 regularization rnn help vanishing gradient problem
225,  Suppose the training error/cost is high and that the validation cost/error is almost equal to it. What does it mean? What should be done?,"Solution: this indicates underfitting. One can add more parameters, increase the complexity of the model, or lower the regularization.",suppose training errorcost high validation costerror almost equal mean done
226,  Describe how L2 regularization can be explained as a sort of a weight decay.,"Solution: Suppose our cost function is C(w), and that we add a penalization c|w|2 . When using gradient descent, the iterations will look likew = w -grad(C)(w) Ã¢ÂÂ 2cw = (1Ã¢ÂÂ2c)w Ã¢ÂÂ grad(C)(w)In this equation, the weight is multiplied by a factor < 1.",l2 regularization explained sort weight decay
227,  What are Support Vector Machines (SVMs)?,"Ã°ÂÂÂ SVM is a supervised machine learning algorithm that works on both classification and regression problem statements.Ã°ÂÂÂ For classification problem statements, it tries to differentiate data points of different classes by finding a hyperplane that maximizes the margin between the classes in the training data.Ã°ÂÂÂ In simple words, SVM tries to choose the hyperplane which separates the data points as widely as possible since this margin maximization improves the modelÃ¢ÂÂs accuracy on the test or the unseen data.Image Source: link",support vector machine svms
228,  What are Support Vectors in SVMs?," Support vectors are those instances that are located on the margin itself. For SVMS, the decision boundary is entirely determined by using only the support vectors.Ã°ÂÂÂ Any instance that is not a support vector (not on the margin boundaries) has no influence whatsoever; you could remove them or add more instances, or move them around, and as long as they stay off the margin they wonÃ¢ÂÂt affect the decision boundary.Ã°ÂÂÂ For computing the predictions, only the support vectors are involved, not the whole training set.",support vector svms
229,  What is the basic principle of a Support Vector Machine?,"ItÃ¢ÂÂs aimed at finding an optimal hyperplane that is linearly separable, and for the dataset which is not directly linearly separable, it extends its formulation by transforming the original data to map into a new space, which is also called kernel trick.",basic principle support vector machine
230,  What are hard margin and soft Margin SVMs?,"Ã°ÂÂÂ ÃÂ Hard margin SVMs work only if the data is linearly separable and these types of SVMs are quite sensitive to the outliers.Ã°ÂÂÂ ÃÂ But our main objective is to find a good balance between keeping the margins as large as possible and limiting the margin violation i.e. instances that end up in the middle of margin or even on the wrong side, and this method is called soft margin SVM.",hard margin soft margin svms
231,  What do you mean by Hinge loss?,"The function defined by max(0, 1 Ã¢ÂÂ t) is called the hinge loss function.Image Source: linkProperties of Hinge loss function:Ã°ÂÂÂ It is equal to 0 when the value of t is greater than or equal to 1 i.e, t>=1.Ã°ÂÂÂ Its derivative (slope) is equal to Ã¢ÂÂ1 if t < 1 and 0 if t > 1.Ã°ÂÂÂ It is not differentiable at t = 1.Ã°ÂÂÂ It penalizes the model for wrongly classifying the instances and increases as far the instance is classified from the correct region of classification.",mean hinge loss
232,  What is the Kernel trick?, A Kernel is a function capable of computing the dot product of instances mapped in higher dimension space without actually transforming all the instances into the higher feature space and calculating the dot product. This trick makes the whole process much less computationally expensive than that actual transformation to calculate the dot product and this is the essence of the kernel trick.Image Source: link,kernel trick
233,  What is the role of the C hyper-parameter in SVM? Does it affect the bias/variance trade-off?,Ã°ÂÂÂ The balance between keeping the margins as large as possible and limiting the margin violation is controlled by the C parameter: a small value leads to a wider street but more margin violation and a higher value of C makes fewer margin violations but ends up with a smaller margin and overfitting.Ã°ÂÂÂ Here thing becomes a little complex as we have conflicting objectives of making the slack variables as small as possible to reduce margin violation and make W as small as possible to increase the margin. This is where the role of the C hyperparameter comes in which allows us to define the trade-off between these two objectives.,role c hyperparameter svm affect biasvariance tradeoff
234,  Explain different types of kernel functions.,"A function is called kernel if there exist a function ÃÂ that maps a and b into another space such that K(a, b) = ÃÂ(a)T ÃÂ· ÃÂ(b). So you can use K as a kernel since you just know that a mapping ÃÂ exists, even if you donÃ¢ÂÂt know what ÃÂ function is. These are the very good things about kernels.Some of the kernel functions are as follows:Ã°ÂÂÂ Polynomial Kernel:ÃÂ These are the kernel functions that represent the similarity of vectors in a feature space over polynomials of original variables.Ã°ÂÂÂ Gaussian Radial Basis Function (RBF) kernel:ÃÂ  Gaussian RBF kernel maps each training instance to an infinite-dimensional space, therefore itÃ¢ÂÂs a good thing that you donÃ¢ÂÂt need to perform the mapping.Image Source: link",different type kernel function
235,  How you formulate SVM for a regression problem statement?,"For formulating SVM as a regression problem statement we have to reverse the objective: instead of trying to fit the largest possible street between two classes which we will do for classification problem statements while limiting margin violations, now for SVM Regression, it tries to fit as many instances as possible between the margin while limiting the margin violations.",formulate svm regression problem statement
236,  What affects the decision boundary in SVM?,"Adding more instances off the margin of the hyperplane does not affect the decision boundary, it is fully determined (or supported ) by the instances located at the edge of the street called support vectors",affect decision boundary svm
237,  What is a slack variable?,"Ã°ÂÂÂ To meet the soft margin objective, we need to introduce a slack variable ÃÂµ>=0 for each sample; it measures how much any particular instance is allowed to violate the margin.Ã°ÂÂÂ Here thing becomes little complex as we have conflicting objectives of making the slack variables as small as possible to reduce margin violation and make w (weight matrix) as small as possible to increase the margin. This is where the role of theÃÂ C hyperparameter comes which allows us to define the trade-off between these two objectives.Fig. Picture Showing the slack variablesImage Source: link",slack variable
238,  What is a dual and primal problem and how is it relevant to SVMs?,"Ã°ÂÂÂ Given a constrained optimization problem, known as the Primal problem, it is possible to express a different but closely related problem, which is known as its Dual problem.Ã°ÂÂÂ The solution to the dual problem typically provides a lower bound to the solution of the primal problem, but under some conditions, it can be possible that it has even the same solutions as the primal problem.Ã°ÂÂÂ Fortunately, the SVM problem completes these conditions, so that you can choose to solve the primal problem or the dual problem; and they both will have the same solution.ÃÂ ",dual primal problem relevant svms
239,  Can an SVM classifier outputs a confidence score when it classifies an instance? What about a probability?,"Ã°ÂÂÂ An SVM classifier can give the distance between the test instance and the decision boundary as output, so we can use that as a confidence score, but we cannot use this score to directly converted it into class probabilities.Ã°ÂÂÂ But if you set probability=True when building a model of SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVMÃ¢ÂÂs scores. By using this techniques, we can add the predict_proba() and predict_log_proba() methods to the SVM model.",svm classifier output confidence score classifies instance probability
240,  If you train an SVM classifier with an RBF kernel. It seems to underfit the training dataset: should you increase or decrease theÃÂ hyper-parameterÃÂ ÃÂ³ (gamma)? What about the CÃÂ hyper-parameter?,"If we trained an SVM classifier using a Radial Basis Function (RBF) kernel, then it underfits the training set, so there might be too much regularization. To decrease it, you need to increase the gamma or C hyper-parameter.",train svm classifier rbf kernel seems underfit training dataset increase decrease theãâ hyperparameterãâ ãâ³ gamma cãâ hyperparameter
241,  Is SVM sensitive to the Feature Scaling?,"Yes, SVMs are sensitive to feature scaling as it takes input data to find the margins around hyperplanes and gets biased for the variance in high values.",svm sensitive feature scaling
242,  Can you explain SVM?,"Explanation: Support vector machines is a supervised machine learning algorithm which works both on classification and regression problems. It tries to classify data by finding a hyperplane that maximizes the margin between the classes in the training data. Hence, SVM is an example of a large margin classifier.The basic idea of support vector machines:",svm
243,  What is the geometric intuition behind SVM?,"Explanation: If you are asked to classify two different classes. There can be multiple hyperplanes which can be drawn.SVM chooses the hyperplane which separates the data points as widely as possible. SVM draws a hyperplane parallel to the actual hyperplane intersecting with the first point of class A (also known as Support Vectors) and another hyperplane parallel to the actual hyperplane intersecting with the first point of class B. SVM tries to maximize these margins. Eventually, this margin maximization improves the modelÃ¢ÂÂs accuracy on unseen data.",geometric intuition behind svm
244,  How would explain Convex Hull in light of SVMs?,Explanation: We simply build a convex hull for class A and class B and draw a perpendicular on the shortest distance between the closest points of both these hulls.,would convex hull light svms
245,  What do know about Hard Margin SVM and Soft Margin SVM?,"Explanation: If a point Xi satisfies the equation Yi(WT*Xi +b) Ã¢ÂÂ¥ 1, then Xi is correctly classified else incorrectly classified. So we can see that if the points are linearly separable then only our hyperplane is able to distinguish between them and if any outlier is introduced then it is not able to separate them. So these type of SVM is called hard margin SVM (since we have very strict constraints to correctly classify each and every data point).To overcome this, we introduce a term ( ÃÂ¾ ) (pronounced as Zeta)if ÃÂ¾i= 0, the points can be considered as correctly classified.if ÃÂ¾i> 0 , Incorrectly classified points.",know hard margin svm soft margin svm
246,  What is Hinge Loss?,"Explanation: Hinge Loss is a loss function which penalises the SVM model for inaccurate predictions.If Yi(WT*Xi +b) Ã¢ÂÂ¥ 1, hinge loss is Ã¢ÂÂ0Ã¢ÂÂ i.e the points are correctly classified. WhenYi(WT*Xi +b) < 1, then hinge loss increases massively.As Yi(WT*Xi +b) increases with every misclassified point, the upper bound of hinge loss {1- Yi(WT*Xi +b)} also increases exponentially.Hence, the points that are farther away from the decision margins have a greater loss value, thus penalising those points.We can formulate hinge loss as max[0, 1- Yi(WT*Xi +b)]",hinge loss
247,  Explain the Dual form of SVM formulation?,"Explanation: The aim of the Soft Margin formulation is to minimizesubject toThis is also known as the primal form of SVM.The duality theory provides a convenient way to deal with the constraints. The dual optimization problem can be written in terms of dot products, thereby making it possible to use kernel functions.It is possible to express a different but closely related problem, called its dual problem. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions, it can even have the same solutions as the primal problem. Luckily, the SVM problem happens to meet these conditions, so you can choose to solve the primal problem or the dual problem; both will have the same solution.",dual form svm formulation
248,  WhatÃ¢ÂÂs the Ã¢ÂÂkernel trickÃ¢ÂÂ and how is it useful?,"Explanation: Earlier we have discussed applying SVM on linearly separable data but it is very rare to get such data. Here, kernel trick plays a huge role. The idea is to map the non-linear separable data-set into a higher dimensional space where we can find a hyperplane that can separate the samples.It reduces the complexity of finding the mapping function. So, Kernel function defines the inner product in the transformed space. Application of the kernel trick is not limited to the SVM algorithm. Any computations involving the dot products (x, y) can utilize the kernel trick.",whatãââs ãââkernel trickãââ useful
249,  What is Polynomial kernel?,"Explanation: Polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.For d-degree polynomials, the polynomial kernel is defined as:",polynomial kernel
250,  What is RBF-Kernel?,"Explanation:The RBF kernel on two samples x and xÃ¢ÂÂ, represented as feature vectors in some input space, is defined as||x-xÃ¢ÂÂ||ÃÂ² recognized as the squared Euclidean distance between the two feature vectors. sigma is a free parameter.",rbfkernel
251,  Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?,"Explanation: This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is proportional to a number between mÃÂ² and mÃÂ³. So, if there are millions of instances, you should use the primal form, because the dual form will be much too slow.",use primal dual form svm problem train model training set million instance hundred feature
252,  Explain about SVM Regression?,"Explanation: The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences. First of all, because the output is a real number it becomes very difficult to predict the information at hand, which has infinite possibilities. In the case of regression, a margin of tolerance (epsilon) is set in approximation to the SVM",svm regression
253,  Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm.,Explanation:,give situation use svm randomforest machine learning algorithm
254,  What is the role of C in SVM? How does it affect the bias/variance trade-off?,"Explanation:In the given Soft Margin Formulation of SVM, C is a hyperparameter.C hyperparameter adds a penalty for each misclassified data point.Large Value of parameter C implies a small margin, there is a tendency to overfit the training model.Small Value of parameter C implies a large margin which might lead to underfitting of the model.",role c svm affect biasvariance tradeoff
255,"  SVM being a large margin classifier, is it influenced by outliers?","Explanation: Yes, if C is large, otherwise not.",svm large margin classifier influenced outlier
256,"  In SVM, what is the angle between the decision boundary and theta?","Explanation: Decision boundary is a plane having equation Theta1*x1+Theta2*x2+Ã¢ÂÂ¦Ã¢ÂÂ¦+c = 0, so as per the property of a plane, itÃ¢ÂÂs coefficients vector is normal to the plane. Hence, the Theta vector is perpendicular to the decision boundary.",svm angle decision boundary theta
257,  Can we apply the kernel trick to logistic regression? Why is it not used in practice then?,Explanation:,apply kernel trick logistic regression used practice
258,  What is the difference between logistic regression and SVM without a kernel?,Explanation: They differ only in the implementation . SVM is much more efficient and has good optimization packages.,difference logistic regression svm without kernel
259,  Can any similarity function be used for SVM?,Explanation: No. It has to have to satisfy MercerÃ¢ÂÂs theorem.,similarity function used svm
260,  Does SVM give any probabilistic output?,"Explanation: SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation",svm give probabilistic output
261,"Generic question for any algorithm: ADA? (Advantages, Disadvantages and Assumptions)","Advantages:Turnkey algorithm, very few parameters to optimize.Optimization is in convex space so a global minimum is reached unlike neural network it doesnÃ¢ÂÂt suffer with problem of stucking at local minima.Memory efficientLinear SVM is more effective than other algorithm if the number of features are highe (>1000)Disadvantages:Feature scalingTends to get slower as number of examples increasesHard margin leads to overfittingAssumptions:None",generic question algorithm ada advantage disadvantage assumption
262,  Explain hard and soft margin in context with SVM.,"Hard Margin Does not allow any errorSoft Margin Allow error, it reduces the chance of overfitting",hard soft margin context svm
263,  Why does SVM gets slower when the number of examples increases?,"The computation cost depends upon the selection of pair of vectors (nC2, as derived in the optimization section).",svm get slower number example increase
264,  What are the hyperparameters associated with SVM?,KernelC: Number of permitted errors. Large C gives a narrow margin and vice versa.,hyperparameters associated svm
265,  Why the SVM algorithm is known to be memory efficient?,"Not all the data points are required to make the decision, once the support vectors are decided only the support vectors and equation of the hyperplane is required to make the decision.",svm algorithm known memory efficient
266,  State the statement is True or False: Ã¢ÂÂSVM decision boundary is perpendicular bisector to the line joining the closest point of convex hull of the two classesÃ¢ÂÂ.,"True statement. Geometrical explanation: make the convex hull of each of the classes, try to join the convex hulls using the closest point and the classification boundary will be the perpendicular bisector.",state statement true false ãââsvm decision boundary perpendicular bisector line joining closest point convex hull two classesãââ
267,  Give a typical example where SVM will be used over RandomForest.,Text classification: SVM is an algorithm of choice for higher-dimensional space when the number of features is large.,give typical example svm used randomforest
268,  What is regularization? How is an SVM algorithm regularized? Is it L1 or L2 regularization in nature?,"Ã¢ÂÂC parameterÃ¢ÂÂ is actually the regularization parameter. The C parameter is multiplied by the sum of errors thus the regularization, in general, is L1 in nature. But if the cost function is modified in such a way that the cost of SVM is C multiplied by the sum of squares of the error the regularization becomes L2 in nature.",regularization svm algorithm regularized l1 l2 regularization nature
269,Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa?,"Ans. The performance depends on many factorsthe number of training instancesthe distribution of the datalinear vs. non-linear problemsinput scale of the featuresthe chosen hyperparametershow you validate/evaluate your modelIn general, It is easier to train a well-performing Random Forest classifier since you have to worry less about hyperparameter optimization. Due to the nature Random Forests, you are less likely to overfit. You simply grow ntrees on n bootstrap samples of the training set on feature subspaces Ã¢ÂÂ using the majority vote, the estimate will be pretty robust.Using Support Vector Machines, you have Ã¢ÂÂmore thingsÃ¢ÂÂ to Ã¢ÂÂworryÃ¢ÂÂ about such as choosing an appropriate kernel (poly, RBF, linear Ã¢ÂÂ¦), the regularization penalty, the regularization strength, kernel parameters such as the poly degree or gamma, and so forth.So, in sum, We can say that Random Forests are much more automated and thus Ã¢ÂÂeasierÃ¢ÂÂ to train compared to SVMs, but there are many examples in literature where SVMs outperform Random Forests and vice versa on different datasets. So, if you like to compare these two, make sure that you run a large enough grid search for the SVM and use nested cross-validation to reduce the performance estimation bias. (https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance)",give situation use svm randomforest machine learning algorithm viceversa
270,  Why SVM is an example of a large margin classifier?,"Why SVM is an example of a large margin classifier?SVM is a type of classifier which classifies positive and negative examples, here blue and red data pointsAs shown in the image, the largest margin is found in order to avoid overfitting ie,.. the optimal hyperplane is at the maximum distance from the positive and negative examples(Equal distant from the boundary lines).To satisfy this constraint, and also to classify the data points accurately, the margin is maximised, that is why this is called the large margin classifier.",svm example large margin classifier
271,  What is the role of C in SVM?,"What is the role of C in SVM?Ans. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.",role c svm
272,  What is the intuition of a large margin classifier?,"What is the intuition of a large margin classifier?Ans. LetÃ¢ÂÂs say youÃ¢ÂÂve found a hyperplane that completely separates the two classes in your training set. We expect that when new data comes along (i.e. your test set), the new data will look like your training data. Points that should be classified as one class or the other should lie near the points in your training data with the corresponding class. Now, if your hyperplane is oriented such that it is close to some of the points in your training set, thereÃ¢ÂÂs a good chance that the new data will lie on the wrong side of the hyperplane, even if the new points lie close to training examples of the correct class.So we say that we want to find the hyperplane with the maximum margin. That is, find a hyperplane that divides your data properly, but is also as far as possible from your data points. That way, when new data comes in, even if it is a little closer to the wrong class than the training points, it will still lie on the right side of the hyperplane.If your data is separable, then there are infinitely many hyperplanes that will separate it. SVM (and some other classifiers) optimizes for the one with the maximum margin, as described above.",intuition large margin classifier
273,  What is a kernel in SVM? Why do we use kernels in SVM?,"What is a kernel in SVM? Why do we use kernels in SVM?Ans. SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis. The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces.",kernel svm use kernel svm
274,  Can we apply the kernel trick to logistic regression? Why is it not used in practice then?,"Can we apply the kernel trick to logistic regression? Why is it not used in practice then?Ans.Looking at the above it almost feels like kernel logistic regression is what you should be using. However, there are certain advantages that SVMs enjoy",apply kernel trick logistic regression used practice
275,  What is the difference between logistic regression and SVM without a kernel?,"What is the difference between logistic regression and SVM without a kernel?Ans. Only in implementation, One is much more efficient and has good optimization packages",difference logistic regression svm without kernel
276,  What is the difference between logistic regression and SVM,"What is the difference between logistic regression and SVMAns. Logistic regression assumes that the predictors arenÃ¢ÂÂt sufficient to determine the response variable, but determine a probability that is a logistic function of a linear combination of them. If thereÃ¢ÂÂs a lot of noise, logistic regression (usually fit with maximum-likelihood techniques) is a great technique.On the other hand, there are problems where you have thousands of dimensions and the predictors do nearly-certainly determine the response, but in some hard-to-explicitly-program way. An example would be image recognition. If you have a grayscale image, 100 by 100 pixels, you have 10,000 dimensions already. With various basis transforms (kernel trick) you will be able to get a linear separator of the data.Non-regularized logistic regression techniques donÃ¢ÂÂt work well (in fact, the fitted coefficients diverge) when thereÃ¢ÂÂs a separating hyperplane, because the maximum likelihood is achieved by any separating plane, and thereÃ¢ÂÂs no guarantee that youÃ¢ÂÂll get the best one. What you get is an extremely confident model with poor predictive power near the margin.SVMs get you the best separating hyperplane, and theyÃ¢ÂÂre efficient in high dimensional spaces. TheyÃ¢ÂÂre similar to regularization in terms of trying to find the lowest-normed vector that separates the data, but with a margin condition that favors choosing a good hyperplane. A hard-margin SVM will find a hyperplane that separates all the data (if one exists) and fail if there is none; soft-margin SVMs (generally preferred) do better when thereÃ¢ÂÂs noise in the data.Additionally, SVMs only consider points near the margin (support vectors). Logistic regression considers all the points in the data set. Which you prefer depends on your problem.Logistic regression is great in a low number of dimensions and when the predictors donÃ¢ÂÂt suffice to give more than a probabilistic estimate of the response. SVMs do better when thereÃ¢ÂÂs a higher number of dimensions, and especially on problems where the predictors do certainly (or near-certainly) determine the responses.",difference logistic regression svm
277,  Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?,"Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?Ans. The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.For a higher gamma, the model will capture the shape of the dataset well.",suppose using rbf kernel svm high gamma value signify
278,  What is generalization error in terms of the SVM?,What is generalization error in terms of the SVM?Ans. Generalisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data.,generalization error term svm
279,  What is the KNN Algorithm?,"KNN(K-nearest neighbours) is a supervised learning and non-parametric algorithm that can be used to solve both classification and regression problem statements.It uses data in which there is a target column present i.e, labelled data to model a function to produce an output for the unseen data. It uses the euclidean distance formula to compute the distance between the data points for classification or prediction.The main objective of this algorithm is that similar data points must be close to each other so it uses the distance to calculate the similar points that are close to each other.Image Source: Google Images",knn algorithm
280,  Why is KNN a non-parametric Algorithm?,"The term Ã¢ÂÂnon-parametricÃ¢ÂÂ refers to not making any assumptions on the underlying data distribution. These methods do not have any fixed numbers of parameters in the model.Similarly in KNN, the model parameters grow with the training data by considering each training case as a parameter of the model. So, KNN is a non-parametric algorithm.",knn nonparametric algorithm
281,  What is k in the KNN Algorithm?,"K represents the number of nearest neighbours you want to select to predict the class of a given item, which is coming as an unseen dataset for the model.ÃÂ ",k knn algorithm
282,  Why is the odd value of K preferred over even values in the KNN Algorithm?,"The odd value of K should be preferred over even values in order to ensure that there are no ties in the voting. If the square root of a number of data points is even, then add or subtract 1 to it to make it odd.ÃÂ ",odd value k preferred even value knn algorithm
283,  How does the KNN algorithm make the predictions on the unseen dataset?,"The following operations have happened during each iteration of the algorithm. For each of the unseen or test data point, the kNN classifier must:Step-1: Calculate the distances of test point to all points in the training set and store themStep-2: Sort the calculated distances in increasing orderStep-3: Store the K nearest points from our training datasetStep-4: Calculate the proportions of each classStep-5: Assign the class with the highest proportionImage Source: Google Images",knn algorithm make prediction unseen dataset
284,  Is Feature Scaling required for the KNN Algorithm? Explain with proper justification.,"Yes, feature scaling is required to get the better performance of the KNN algorithm.For Example, Imagine a dataset having n number of instances and N number of features. There is one feature having values ranging between 0 and 1. Meanwhile, there is also a feature that varies from -999 to 999. When these values are substituted in the formula of Euclidean Distance, this will affect the performance by giving higher weightage to variables having a higher magnitude.ÃÂ ",feature scaling required knn algorithm proper justification
285,  What is space and time complexity of the KNN Algorithm?,"Time complexity:The distance calculation step requires quadratic time complexity, and the sorting of the calculated distances requires an O(N log N) time. Together, we can say that the process is an O(N3 log N) process, which is a monstrously long process.Space complexity:Since it stores all the pairwise distances and is sorted in memory on a machine, memory is also the problem. Usually, local machines will crash, if we have very large datasets.",space time complexity knn algorithm
286,  Can the KNN algorithm be used for regression problem statements?,"Yes, KNN can be used for regression problem statements.In other words, the KNN algorithm can be applied Ã¢ÂÂwhen the dependent variable is continuous. For regression problem statements, the predicted value is given by the average of the values of its k nearest neighbours.",knn algorithm used regression problem statement
287,  Why is the KNN Algorithm known as Lazy Learner?,"When the KNN algorithm gets the training data, it does not learn and make a model, it just stores the data. Instead of finding any discriminative function with the help of the training data, it follows instance-based learning and also uses the training data when it actually needs to do some prediction on the unseen datasets.As a result, KNN does not immediately learn a model rather delays the learning thereby being referred to as Lazy Learner.",knn algorithm known lazy learner
288,  Why is it recommended not to use the KNN Algorithm for large datasets?,"The Problem in processing the data:KNN works well with smaller datasets because it is a lazy learner. It needs to store all the data and then make a decision only at run time. It includes the computation of distances for a given point with all other points. So if the dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm.Sensitive to noise:Another thing in the context of large datasets is that there is more likely a chance of noise in the dataset which adversely affects the performance of the KNN algorithm since the KNN algorithm is sensitive to the noise present in the dataset.",recommended use knn algorithm large datasets
289,  How to handle categorical variables in the KNN Algorithm?,"To handle the categorical variables we have to create dummy variables out of a categorical variable and include them instead of the original categorical variable. Unlike regression, create k dummies instead of (k-1).For example, a categorical variable named Ã¢ÂÂDegreeÃ¢ÂÂ has 5 unique levels or categories. So we will create 5 dummy variables. Each dummy variable has 1 against its degree and else 0.",handle categorical variable knn algorithm
290,  How to choose the optimal value of K in the KNN Algorithm?,"There is no straightforward method to find the optimal value of K in the KNN algorithm.You have to play around with different values to choose which value of K should be optimal for my problem statement. Choosing the right value of K is done through a process known as Hyperparameter Tuning.The optimum value of K for KNN is highly dependent on the data itself. In different scenarios, the optimum K may vary. It is more or less a hit and trial method.There is no one proper method of finding the K value in the KNN algorithm. No method is the rule of thumb but you should try the following suggestions:",choose optimal value k knn algorithm
291,"  Cross-Validation Method: We should also take the help of cross-validation to find out the optimal value of K in KNN. Start with the minimum value of k i.e, K=1, and run cross-validation, measure the accuracy, and keep repeating till the results become consistent.","As the value of K increases, the error usually goes down after each one-step increase in K, then stabilizes, and then raises again. Finally, pick the optimum K at the beginning of the stable zone. This technique is also known as the Elbow Method. Image Source: Google Images",crossvalidation method also take help crossvalidation find optimal value k knn start minimum value k ie k1 run crossvalidation measure accuracy keep repeating till result become consistent
292,  Domain Knowledge: Sometimes with the help of domain knowledge for a particular use case we are able to find the optimum value of K (K should be an odd number).,I would therefore suggest trying a mix of all the above points to reach any conclusion.,domain knowledge sometimes help domain knowledge particular use case able find optimum value k k odd number
293,  How can you relate KNN Algorithm to the Bias-Variance tradeoff?,"Problem with having too small K:The major concern associated with small values of K lies behind the fact that the smaller value causes noise to have a higher influence on the result which will also lead to a large variance in the predictions.Problem with having too large K:The larger the value of K, the higher is the accuracy. If K is too large, then our model is under-fitted. As a result, the error will go up again. So, to prevent your model from under-fitting it should retain the generalization capabilities otherwise there are fair chances that your model may perform well in the training data but drastically fail in the real data. The computational expense of the algorithm also increases if we choose the k very large.So, choosing k to a large value may lead to a model with a large bias(error).The effects of k values on the bias and variance is explained below :So, there is a tradeoff between overfitting and underfitting and you have to maintain a balance while choosing the value of K in KNN. Therefore, K should not be too small or too large.ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ ÃÂ  Image Source: Google Images",relate knn algorithm biasvariance tradeoff
294,  Which algorithm can be used for value imputation in both categorical and continuous categories of data?,"KNN is the only algorithm that can be used for the imputation of both categorical and continuous variables. It can be used as one of many techniques when it comes to handling missing values.To impute a new sample, we determine the samples in the training set Ã¢ÂÂnearestÃ¢ÂÂ to the new sample and averages the nearby points to impute. A Scikit learn library of PythonÃÂ provides a quick and convenient way to use this technique.Note: NaNs are omitted while distances are calculated. Hence we replace the missing values with the average value of the neighbours. The missing values will then be replaced by the average value of their Ã¢ÂÂneighboursÃ¢ÂÂ. 15. Explain the statement- Ã¢ÂÂThe KNN algorithm does more computation on test time rather than train timeÃ¢ÂÂ.The above-given statement is absolutely true.The basic idea behind the kNN algorithm is to determine a k-long list of samples that are close to a sample that we want to classify. Therefore, the training phase is basically storing a training set, whereas during the prediction stage the algorithm looks for k-neighbours using that stored data. Moreover, KNN does not learn anything from the training dataset as well.",algorithm used value imputation categorical continuous category data
295,  What are the things which should be kept in our mind while choosing the value of k in the KNN Algorithm?,"If K is small, then results might not be reliable because the noise will have a higher influence on the result. If K is large, then there will be a lot of processing to be done which may adversely impact the performance of the algorithm.So, the following things must be considered while choosing the value of K:",thing kept mind choosing value k knn algorithm
296,"  No Training Period: It does not learn anything during the training period since it does not find any discriminative function with the help of the training data. In simple words, actually, there is no training period for the KNN algorithm. It stores the training dataset and learns from it only when we use the algorithm for making the real-time predictions on the test dataset.","As a result, the KNN algorithm is much faster than other algorithms which require training.ÃÂ For Example, SupportVector Machines(SVMs), Linear Regression, etc.Moreover, since the KNN algorithm does not require any training before making predictions as a result new data can be added seamlessly without impacting the accuracy of the algorithm.",training period learn anything training period since find discriminative function help training data simple word actually training period knn algorithm store training dataset learns use algorithm making realtime prediction test dataset
297,  Is it possible to use the KNN algorithm for Image processing? ,"Yes, KNN can be used for image processing by converting a 3-dimensional image into a single-dimensional vector and then using it as the input to the KNN algorithm.",possible use knn algorithm processing
298,What is the Central Limit Theorem and why is it important?,"Suppose that we are interested in estimating the average height among all people. Collecting data for ÃÂ every person in the world is impossible. While we canÃ¢ÂÂt obtain a height measurement from everyone in the ÃÂ population, we can still sample some people. The question now becomes, what can we say about the ÃÂ average height of the entire population given a single sample. The Central Limit Theorem addresses this ÃÂ question exactly.",central limit theorem important
299,What is sampling? How many sampling methods do you know?,"Data sampling is a statistical analysis technique used to select, manipulate and analyze a representative ÃÂ subset of data points to identify patterns and trends in the larger data set being examined",sampling many sampling method know
300,What is the difference between type I vs type II error?,"A type I error occurs when the null hypothesis is true, but is rejected. A type II error occurs when the null ÃÂ hypothesis is false, but erroneously fails to be rejected.",difference type v type ii error
301,"What is linear regression? What do the terms p-value, coefficient, and r-squared ÃÂ value mean? What is the significance of each of these components?ÃÂ ","A linear regression is a good tool for quick predictive analysis: for example, the price of a house depends ÃÂ on a myriad of factors, such as its size or its location. In order to see the relationship between these ÃÂ variables, we need to build a linear regression, which predicts the line of best fit between them and can ÃÂ help conclude whether or not these two factors have a positive or negative relationship.",linear regression term pvalue coefficient rsquared ãâ value mean significance componentsãâ
302,What are the assumptions required for linear regression?,"There are four major assumptions: 1. There is a linear relationship between the dependent variables and ÃÂ the regressors, meaning the model you are creating actually fits the data, 2. The errors or residuals of the ÃÂ data are normally distributed and independent from each other, 3. There is minimal multicollinearity ÃÂ between explanatory variables, and 4. Homoscedasticity. This means the variance around the regression ÃÂ line is the same for all values of the predictor variable",assumption required linear regression
303,What is a statistical interaction?,"Basically, an interaction is when the effect of one factor (input variable) on the dependent variable (output ÃÂ variable) differs among levels of another factor",statistical interaction
304,What is selection bias?,"Selection (or Ã¢ÂÂsamplingÃ¢ÂÂ) bias occurs in an Ã¢ÂÂactive,Ã¢ÂÂ sense when the sample data that is gathered and ÃÂ prepared for modeling has characteristics that are not representative of  . the true, future population of cases",selection bias
305,What is the Binomial Probability Formula? ,The binomial distribution consists of the probabilities of each of the possible numbers of successes on N ÃÂ trials for independent events that each have a probability of ? (the Greek letter pi) of occurring.,binomial probability formula
306,What is Selection Bias? ,"Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It isÃÂ  usually associated with research where the selection of participants  as the selection effect. It is the distortion of statistical analysis, resulting from the method of collecting  samples. If the selection bias is not taken into account, then some conclusions of the study may not be  accurate.  isnÃ¢ÂÂt random. It is sometimes referred to",selection bias
307,What is a confusion matrix? ,"The confusion matrix is a 2X2 table that contains 4 outputs provided by the binary classifier. VariousÃÂ  measures, such as error-rate, accuracy, specificity, sensitivity, precision and recall are derived fromÃÂ  it. Confusion Matrix ",confusion matrix
308,What do you understand by the term Normal Distribution?,"Data is usually distributed in different ways with a bias to the left or to the right or it can all be jumbled up.However, there are chances that data is distributed around a central value without any bias to the left or rightÃÂ  and reaches normal distribution in the form of a bell-shaped curve.ÃÂ ",understand term normal distribution
309,What is correlation and covariance in statistics?,"Covariance and Correlation are two mathematical concepts; these two approaches are widely used inÃÂ  statistics. Both Correlation and Covariance establish the relationship and also measure the dependencyÃÂ  between two random variables. Though the work is similar between these two in mathematical terms, theyÃÂ  are different from each other. Covariance: In covariance two items vary together and itÃ¢ÂÂs a measure that indicates the extent to which two  random variables change in cycle. It is a statistical term; it explains the systematic relation between a pair of  random variables, wherein changes in one variable reciprocal by a corresponding change in another  variable.",correlation covariance statistic
310,What is an example of a data set with a non-Gaussian distribution?,"The Gaussian distribution is part of the Exponential family of distributions, but there are a lot more of ÃÂ them, with the same sort of ease of use, in many cases, and if the person doing the machine learning has ÃÂ a solid grounding in statistics, they can be utilized where appropriate.",example data set nongaussian distribution
311,What is the difference between Point Estimates and Confidence Interval?,"Point Estimation gives us a particular value as an estimate of a population parameter. Method of MomentsÃÂ  and Maximum Likelihood estimator methods are used to derive Point Estimators for population parameters. A confidence interval gives us a range of values which is likely to contain the population parameter. TheÃÂ  confidence interval is generally preferred, as it tells us how likely this interval is to contain the populationÃÂ  parameter. This likeliness or probability is called Confidence Level or Confidence coefficient and representedÃÂ  by 1 Ã¢ÂÂ alpha, where alpha is the level of significance.",difference point estimate confidence interval
312,What is the goal of A/B Testing?,It is a hypothesis testing for a randomized experiment with two variables A and B. The goal of A/B Testing is to identify any changes to the web page to maximize or increase the outcome ofÃÂ  interest. A/B testing is a fantastic method for figuring out the best online promotional and marketing strategiesÃÂ  for your business.,goal ab testing
313,What is p-value?,"When you perform a hypothesis test in statistics, a p-value can help you determine the strength of yourÃÂ  results. p-value is a number between 0 and 1. Based on the value it will denote the strength of the results.ÃÂ  The claim which is on trial is called the Null Hypothesis. Low p-value (? 0.05) indicates strength against the null hypothesis which means we can reject the nullÃÂ  Hypothesis. High p-value (? 0.05) indicates strength for the null hypothesis which means we can accept theÃÂ  null Hypothesis p-value of 0.05 indicates the Hypothesis could go either way. To put it in another way, High P values: your data are likely with a true null. Low P values: your data are unlikely with a true null. ",pvalue
314,What do you understand by statistical power of sensitivity and how do you calculate it?,"Sensitivity is commonly used to validate the accuracy of a classifier (Logistic, SVM, Random Forest etc.) Sensitivity is nothing but Ã¢ÂÂPredicted True events/ Total eventsÃ¢ÂÂ. True events here are the events which wereÃÂ  true and model also predicted them as true. Calculation of seasonality is pretty straightforward. Seasonality = ( True Positives ) / ( Positives in Actual Dependent Variable ) ",understand statistical power sensitivity calculate
315,Why Is Re-sampling Done? ,"Resampling is done in any of these cases: Estimating the accuracy of sample statistics by using subsets of accessible data or drawing randomlyÃÂ  with replacement from a set of data points. Substituting labels on data points when performing significance tests. Validating models by using random subsets (bootstrapping, cross-validation)ÃÂ ", resampling done
316,What are the differences between over-fitting and under-fitting?,"In overfitting, a statistical model describes random error or noise instead of the underlying relationship.ÃÂ  Overfitting occurs when a model is excessively complex, such as having too many parameters relative toÃÂ  the number of observations. A model that has been overfitted, has poor predictive performance, as itÃÂ  overreacts to minor fluctuations in the training data. Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlyingÃÂ  trend of the data. Underfitting would occur, for example, when fitting a linear model to non-linear data. SuchÃÂ  a model too would have poor predictive performance.",difference overfitting underfitting
317,How to combat Overfitting and Underfitting?,"To combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-foldÃÂ  cross-validation) and by having a validation dataset to evaluate the model. ", combat overfitting underfitting
318,What is regularisation? Why is it useful?,Regularisation is the process of adding tuning parameter to a model to induce smoothness in order to preventÃÂ  overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constantÃÂ  is often the L1(Lasso) or L2(ridge). The model predictions should then minimize the loss function calculatedÃÂ  on the regularized training set. ,regularisation useful
319,What Is the Law of Large Numbers?,"It is a theorem that describes the result of performing the same experiment a large number of times. ThisÃÂ  theorem forms the basis of frequency-style thinking. It says that the sample means, the sample varianceÃÂ  and the sample standard deviation converge to what they are trying to estimate.",law large number
320,What Are Confounding Variables?,"In statistics, a confounder is a variable that influences both the dependent variable and independent variable.A confounding variable here would be any other variable that affects both of these variables, such as the ageÃÂ  of the subject. ",confounding variable
321,What Are the Types of Biases That Can Occur During Sampling?,"Selection bias, Under coverage bias, Survivorship bias. ", type bias occur sampling
322,What is Survivorship Bias?,It is the logical error of focusing aspects that support surviving some process and casually overlooking thoseÃÂ  that did not work because of their lack of prominence. This can lead to wrong conclusions in numerousÃÂ  different means. ,survivorship bias
323,Explain how a ROC curve works?,The ROC curve is a graphical representation of the contrast between true positive rates and false-positiveÃÂ  rates at various thresholds. It is often used as a proxy for the trade-off between the sensitivity(true positiveÃÂ  rate) and false-positive rate.,roc curve work
324,What is TF/IDF vectorization?,"""TFIDF is short for term frequency-inverse document frequency, is a numerical statistic that is intended toÃÂ  reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factorÃÂ  in information retrieval and text mining. The TFÃ¢ÂÂIDF value increases proportionally to the number of times a word appears in the document but isÃÂ  offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appearÃÂ  more frequently in general.",tfidf vectorization
325,Python or R  Which one would you prefer for text analytics? ,We will prefer Python because of the following reasons: ÃÂ Python would be the best option because it has Pandas library that provides easy to use dataÃÂ  structures and high-performance data analysis tools. R is more suitable for machine learning than just text analysis. ÃÂ Python performs faster for all types of text analytics.,python r  one would prefer text analytics
326,How does data cleaning plays a vital role in the analysis?,"Data cleaning can help in analysis because: Cleaning data from multiple sources helps to transform it into a format that data analysts or dataÃÂ  scientists can work with. Data Cleaning helps to increase the accuracy of the model in machine learning. It is a cumbersome process because as the number of data sources increases, the time taken toÃÂ  clean the data increases exponentially due to the number of sources and the volume of dataÃÂ  generated by these sources. It might take up to 80% of the time for just cleaning data ",data cleaning play vital role analysis
327,"Differentiate between univariate, bivariate and multivariate analysis.","Univariate analyses are descriptive statistical analysis techniques which can be differentiated based on theÃÂ  number of variables involved at a given point of time. For example, the pie charts of sales based on territoryÃÂ  involve only one variable and can the analysis can be referred to as univariate analysis. The bivariate analysis attempts to understand the difference between two variables at a time as in aÃÂ  scatterplot. For example, analyzing the volume of sale and spending can be considered as an example ofÃÂ  bivariate analysis. Multivariate analysis deals with the study of more than two variables to understand the effect of variablesÃÂ  on the responses.",differentiate univariate bivariate multivariate analysis
328,What is Cluster Sampling? ,"Cluster sampling is a technique used when it becomes difficult to study the target population spread acrossÃÂ  a wide area and simple random sampling cannot be applied. Cluster Sample is a probability sample whereÃÂ  each sampling unit is a collection or cluster of elements. For eg., A researcher wants to survey the academic performance of high school students in Japan. He canÃÂ  divide the entire population of Japan into different clusters (cities). Then the researcher selects a number ofÃÂ  clusters depending on his research through simple or systematic random sampling. ",cluster sampling
329,What is Systematic Sampling?,"Systematic sampling is a statistical technique where elements are selected from an ordered sampling frame.ÃÂ  In systematic sampling, the list is progressed in a circular manner so once you reach the end of the list, it isÃÂ  progressed from the top again. The best example of systematic sampling is equal probability method.",systematic sampling
330,What are Eigenvectors and Eigenvalues?,"Eigenvectors are used for understanding linear transformations. In data analysis, we usually calculate theÃÂ  eigenvectors for a correlation or covariance matrix. Eigenvectors are the directions along which a particularÃÂ  linear transformation acts by flipping, compressing or stretching. Eigenvalue can be referred to as the strength of the transformation in the direction of eigenvector or theÃÂ  factor by which the compression occurs. ",eigenvectors eigenvalue
331,Can you explain the difference between a Validation Set and a Test Set? ,"A Validation set can be considered as a part of the training set as it is used for parameter selection and toÃÂ  avoid overfitting of the model being built. On the other hand, a Test Set is used for testing or evaluating the performance of a trained machine learningÃÂ  model. In simple terms, the differences can be summarized as; training set is to fit the parameters i.e. weights andÃÂ  test set is to assess the performance of the model i.e. evaluating the predictive power and generalization. ",difference validation set test set
332,Explain cross-validation. ,Cross-validation is a model validation technique for evaluating how the outcomes of statistical analysisÃÂ  will generalize to an independent dataset. Mainly used in backgrounds where the objective is forecast andÃÂ  one wants to estimate how accurately a model will accomplish in practice. The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation dataÃÂ  set) in order to limit problems like overfitting and get an insight on how the model will generalize to anÃÂ  independent data set. ,crossvalidation
333,What is Naive in a Naive Bayes?,"The Naive Bayes Algorithm is based on the Bayes Theorem. BayesÃ¢ÂÂ theorem describes the probability ofÃÂ  an event, based on prior knowledge of conditions that might be related to the event. The Algorithm is Ã¢ÂÂnaiveÃ¢ÂÂ because it makes assumptions that may or may not turn out to be correct.",nave  naive bayes
334,Explain SVM algorithm in detail. ,"SVM stands for support vector machine, it is a supervised machine learning algorithm which can be usedÃÂ  for both Regression and Classification. If you have n features in your training data set, SVM tries to plot it in n-dimensional space with the value of each feature being the value of a particular coordinate. SVM uses hyperplanes to separate out different classes based on the provided kernel function. ",svm algorithm
335,What are the different kernels in SVM?,"There are four types of kernels in SVM. Linear Kernel, Polynomial kernel, Radial basis kernel, Sigmoid kernel.",different kernel svm
336,Explain Decision Tree algorithm in detail.,A decision tree is a supervised machine learning algorithm mainly used for Regression andÃÂ  Classification. It breaks down a data set into smaller and smaller subsets while at the same time anÃÂ  associated decision tree is incrementally developed. The final result is a tree with decision nodes and leafÃÂ  nodes. A decision tree can handle both categorical and numerical data. ,decision tree algorithm
337,What is pruning in Decision Tree?,"Pruning is a technique in machine learning and search algorithms that reduces the size of decisionÃÂ  trees by removing sections of the tree that provide little power to classify instances. So, when we removeÃÂ  sub-nodes of a decision node, this process is called pruning or opposite process of splitting. ",pruning decision tree
338,What is logistic regression?,Logistic Regression often referred to as the logit model is a technique to predict the binary outcome fromÃÂ  a linear combination of predictor variables. ,logistic regression
339,What Are the Drawbacks of the Linear Model? ,"Some drawbacks of the linear model are: The assumption of linearity of the errors, ÃÂ It canÃ¢ÂÂt be used for count outcomes or binary outcomes, There are overfitting problems that it canÃ¢ÂÂt solve.", drawback linear model
340,What is the difference between Regression and classification ML techniques?,"Both Regression and classification machine learning techniques come under Supervised machineÃÂ  learning algorithms. In Supervised machine learning algorithm, we have to train the model using labelledÃÂ  data set, While training we have to explicitly provide the correct labels and algorithm tries to learn the patternÃÂ  from input to output. If our labels are discrete values then it will a classification problem,", difference regression classification ml technique
341,What are Recommender Systems?,"Recommender Systems are a subclass of information filtering systems that are meant to predict theÃÂ  preferences or ratings that a user would give to a product. Recommender systems are widely used in movies,ÃÂ  news, research articles, products, social tags, music, etc. ",recommender system
342,What is Collaborative filtering?,"The process of filtering used by most of the recommender systems to find patterns or information byÃÂ  collaborating viewpoints, various data sources and multiple agents.", collaborative filtering
343,How can outlier values be treated?,"Outlier values can be identified by using univariate or any other graphical analysis method. If the number ofÃÂ  outlier values is few then they can be assessed individually but for a large number of outliers, the values canÃÂ  be substituted with either the 99th or the 1st percentile values. All extreme values are not outlier values. The most common ways to treat outlier values. To change the value and bring it within a range. To just remove the value.",outlier value treated
344,"During analysis, how do you treat missing values?","The extent of the missing values is identified after identifying the variables with missing values. If anyÃÂ  patterns are identified the analyst has to concentrate on them as it could lead to interesting and meaningfulÃÂ  business insights. If there are no patterns identified, then the missing values can be substituted with mean or median valuesÃÂ  (imputation) or they can simply be ignored. Assigning a default value which can be mean, minimum orÃÂ  maximum value. Getting into the data is important. If it is a categorical variable, the default value is assigned. The missing value is assigned a default value. IfÃÂ  you have a distribution of data coming, for normal distribution give the mean value. If 80% of the values for a variable are missing then you can answer that you would be dropping the variableÃÂ  instead of treating the missing values.", analysis treat missing value
345,What is Ensemble Learning?,Ensemble Learning is basically combining a diverse set of learners(Individual models) together to improviseÃÂ  on the stability and predictive power of the model.ÃÂ , ensemble learning
346,What is bagging?,"Bagging tries to implement similar learners on small sample populations and then takes a mean of all theÃÂ  predictions. In generalised bagging, you can use different learners on different population. As you expectÃÂ  this helps us to reduce the variance error.",bagging
347,What is boosting?,"Boosting is an iterative technique which adjusts the weight of an observation based on the lastÃÂ  classification. If an observation was classified incorrectly, it tries to increase the weight of this observationÃÂ  and vice versa. Boosting in general decreases the bias error and builds strong predictive models. However,ÃÂ  they may over fit on the training data. ",boosting
348,What is a Box-Cox Transformation?,"The dependent variable for a regression analysis might not satisfy one or more assumptions of an ordinaryÃÂ  least squares regression. The residuals could either curve as the prediction increases or follow the skewedÃÂ  distribution. In such scenarios, it is necessary to transform the response variable so that the data meetsÃÂ  the required assumptions. A Box cox transformation is a statistical technique to transform non-normalÃÂ  dependent variables into a normal shape. If the given data is not normal then most of the statistical techniques assume normality. Applying a box cox transformation means that you can run a broaderÃÂ  number of tests.",boxcox transformation
349,What do you mean by Deep Learning?,Deep Learning is nothing but a paradigm of machine learning which has shown incredible promise in recentÃÂ  years. This is because of the fact that Deep Learning shows a great analogy with the functioning of theÃÂ  human brain. ,mean deep learning
350,What are Artificial Neural Networks?,Artificial Neural networks are a specific set of algorithms that have revolutionized machine learning. TheyÃÂ  are inspired by biological neural networks. Neural Networks can adapt to changing the input so the networkÃÂ  generates the best possible result without needing to redesign the output criteria. , artificial neural network
351,Describe the structure of Artificial Neural Networks?,"Artificial Neural Networks works on the same principle as a biological Neural Network. It consists of inputsÃÂ  which get processed with weighted sums and Bias, with the help of Activation Functions.",structure artificial neural network
352,How Are Weights Initialized in a Network?,"There are two methods here: we can either initialize the weights to zero or assign them randomly.Initializing all weights to 0: This makes your model similar to a linear model. All the neurons and every layerÃÂ  perform the same operation, giving the same output and making the deep net useless. Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close toÃÂ  0. It gives better accuracy to the model since every neuron performs different computations. This is the mostÃÂ  commonly used method. ",weight initialized network
353,What Are Hyperparameters? ,"With neural networks, you're usually working with hyperparameters once the data is formatted correctly. AÃÂ  hyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate,ÃÂ  epochs, etc.).",hyperparameters
354,What is Epoch?,It represent one iteration over the entire dataset (everything put into the training model). ,epoch
355,What is Batch?,"It refers to when we cannot pass the entire dataset into the neural network at once, so we  divide the dataset into several batches.",batch
356,What are Recurrent Neural Networks(RNNs)?,"RNNs are a type of artificial neural networks designed to recognise the pattern from the sequence of dataÃÂ  such as Time series, stock market and government agencies etc. To understand recurrent nets, first, youÃÂ  have to understand the basics of feedforward nets. Both these networks RNN and feed-forward named after the way they channel information through a seriesÃÂ  of mathematical orations performed at the nodes of the network. One feeds information throughÃÂ  straight(never touching the same node twice), while the other cycles it through a loop, and the latter areÃÂ  called recurrent.",recurrent neural networksrnns
357,What Is a Multi-layer Perceptron(MLP)?,"As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has theÃÂ  same structure as a single layer perceptron with one or more hidden layers. A single layer perceptron canÃÂ  classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes. Except for the input layer, each node in the otherÃÂ  layers uses a nonlinear activation function. This means the input layers, the data coming in, and theÃÂ  activation function is based upon all nodes and weights being added together, producing the output. MLPÃÂ  uses a supervised learning method called Ã¢ÂÂbackpropagation.Ã¢ÂÂ In backpropagation, the neural networkÃÂ  calculates the error with the help of cost function. It propagates this error backward from where it cameÃÂ  (adjusts the weights to train the model more accurately).",multilayer perceptronmlp
358,What is exploding gradients?,"While training an RNN, if you see exponentially growing (very large) error gradients which accumulateÃÂ  and result in very large updates to neural network model weights during training, theyÃ¢ÂÂre known asÃÂ  exploding gradients. At an extreme, the values of weights can become so large as to overflow and result inÃÂ  NaN values. This has the effect of your model is unstable and unable to learn from your training data. ",is exploding gradient
359,What is vanishing gradients? ,"While training an RNN, your slope can become either too small; this makes the training difficult. When theÃÂ  slope is too small, the problem is known as a Vanishing Gradient. It leads to long training times, poor performance, and low accuracy. ",vanishing gradient
360,What is Back Propagation?,"Backpropagation is a training algorithm used for multilayer neural network. In this method, we move theÃÂ  error from an end of the network to all weights inside the network and thus allowing efficient computation ofÃÂ  the gradient. ",back propagation
361,What is the role of the Activation Function?,The Activation function is used to introduce non-linearity into the neural network helping it to learn moreÃÂ  complex function. Without which the neural network would be only able to learn linear function which is aÃÂ  linear combination of its input data. An activation function is a function in an artificial neuron that delivers anÃÂ  output based on inputs.,role activation function
362,What is an Auto-Encoder? ,"Auto-encoders are simple learning networks that aim to transform inputs into outputs with the minimumÃÂ  possible error. This means that we want the output to be as close to input as possible. We add a couple ofÃÂ  layers between the input and the output, and the sizes of these layers are smaller than the input layer. TheÃÂ  auto-encoder receives unlabelled input which is then encoded to reconstruct the input. ",autoencoder
363,What is a Boltzmann Machine?,Boltzmann machines have a simple learning algorithm that allows them to discover interesting features thatÃÂ  represent complex regularities in the training data. The Boltzmann machine is basically used to optimise theÃÂ  weights and the quantity for the given problem. The learning algorithm is very slow in networks with manyÃÂ  layers of feature detectors. Ã¢ÂÂRestricted Boltzmann MachinesÃ¢ÂÂ algorithm has a single layer of featureÃÂ  detectors which makes it faster than the rest.,boltzmann machine
364,What Is Dropout and Batch Normalization? ,Dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfittingÃÂ  of data (typically dropping 20 per cent of the nodes). It doubles the number of iterations needed to convergeÃÂ  the network. ,dropout batch normalization
365,Why Is Tensorflow the Most Preferred Library in Deep Learning? ,"Tensorflow provides both C++ and Python APIs, making it easier to work on and has a faster compilationÃÂ  time compared to other Deep Learning libraries like Keras and Torch. Tensorflow supports both CPU andÃÂ  GPU computing devices. ", tensorflow preferred library deep learning
366,What Do You Mean by Tensor in Tensorflow?,A tensor is a mathematical object represented as arrays of higher dimensions. These arrays of data withÃÂ  different dimensions and ranks fed as input to the neural network are called Ã¢ÂÂTensors.Ã¢ÂÂ,mean tensor tensorflow
367,What is the Computational Graph?,"Everything in a tensorflow is based on creating a computational graph. It has a network of nodes where eachÃÂ  node operates, Nodes represent mathematical operations, and edges represent tensors. Since data flowsÃÂ  in the form of a graph, it is also called a Ã¢ÂÂDataFlow Graph.Ã¢ÂÂ", computational graph
368,What are dimensionality reduction and its benefits?,"Dimensionality reduction refers to the process of converting a data set with vast dimensions into data ÃÂ with fewer dimensions (fields) to convey similar information concisely. This reduction helps in compressing data and reducing storage space. It also reduces computation time as ÃÂ fewer dimensions lead to less computing. It removes redundant features; for example, there's no point in ÃÂ storing a value in two different units (meters and inches). ",dimensionality reduction benefit
369,How can you select k for k-means?,"We use the elbow method to select k for k-means clustering. The idea of the elbow method is to run k means clustering on the data set where 'k' is the number of clusters.Within the sum of squares (WSS), it is defined as the sum of the squared distance between each member ÃÂ of the cluster and its centroid.",select k kmeans
370,What are the feature vectors?,"A feature vector is an n-dimensional vector of numerical features that represent an object. In machine ÃÂ learning, feature vectors are used to represent numeric or symbolic characteristics (called features) of an ÃÂ object in a mathematical way that's easy to analyze.",feature vector
371,What is root cause analysis?,Root cause analysis was initially developed to analyze industrial accidents but is now widely used in other ÃÂ areas. It is a problem-solving technique used for isolating the root causes of faults or problems. A factor is ÃÂ called a root cause if its deduction from the problem-fault-sequence averts the final undesirable event from ÃÂ recurring. ,root cause analysis
372,What is collaborative filtering?,"Most recommender systems use this filtering process to find patterns and information by collaborating ÃÂ perspectives, numerous data sources, and several agents.",collaborative filtering
373,What is supervised learning?,"Supervised learning is an approach to creating artificial intelligence (AI), where a computer algorithm is trained on input data that has been labeled for a particular output. The model is trained until it can detect the underlying patterns and relationships between the input data and the output labels, enabling it to yield accurate labeling results when presented with never-before-seen data.Supervised learning is good at classification and regression problems, such as determining what category a news article belongs to or predicting the volume of sales for a given future date. In supervised learning, the aim is to make sense of data within the context of a specific question.",supervised learning
374,How does supervised learning work?,"Like allÂ machine learningÂ algorithms, supervised learning is based on training. During its training phase, the system is fed with labeled data sets, which instruct the system what output is related to each specific input value. The trained model is then presented with test data: This is data that has been labeled, but the labels have not been revealed to the algorithm. The aim of the testing data is to measure how accurately the algorithm will perform on unlabeled data.",supervised learning work
375,List some Supervised learning algorithms,"Neural networks,Naive Bayes,Linear regression,Logistic regression,Support vector machine (SVM),K-nearest neighbor,Random forest.",supervised learning algorithm
376,What is Naive Bayes algorithm?,"It is aÂ classification techniqueÂ based onÂ BayesÂ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ÂNaiveÂ.Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known toÂ outperform even highlyÂ sophisticated classification methods.",naive bayes algorithm
377,What is the Decision Tree Algorithm?,A Decision Tree is a supervised machine learning algorithm that can be used for both Regression and Classification problem statements. It divides the complete dataset into smaller subsets while at the same time an associated Decision Tree is incrementally developed.The final output of the Decision Trees is a Tree having Decision nodes and leaf nodes. A Decision Tree can operate on both categorical and numerical data.,decision tree algorithm
378,Explain the CART Algorithm for Decision Trees.,"The CART stands forÂ Classification and Regression TreesÂ is a greedy algorithm that greedily searches for an optimum split at the top level, then repeats the same process at each of the subsequent levels.Moreover, it does verify whether the split will lead to the lowest impurity or not as well as the solution provided by the greedy algorithm is not guaranteed to be optimal, it often produces a solution thatÂs reasonably good since finding the optimal Tree is anÂ NP-Complete problemÂ that requiresÂ exponential time complexity.As a result, it makes the problem intractable even for small training sets. This is why we must go for aÂ Âreasonably goodÂÂ solution instead of an optimal solution.",cart algorithm decision tree
379,Which should be preferred among Gini impurity and Entropy?,"In reality, most of the time it does not make a big difference: they lead to almost similar Trees. Gini impurity is a good default while implementing in sklearn since it is slightly faster to compute.However, when they work in a different way, then Gini impurity tends to isolate the most frequent class in its own branch of the Tree, while entropy tends to produce slightly more balanced Trees.",preferred among gini impurity entropy
380,List down the different types of nodes in Decision Trees.,The Decision Tree consists of the following different types of nodes:1. Root node:Â It is the top-most node of the Tree from where the Tree starts.2. Decision nodes:Â One or more Decision nodes that result in the splitting of data into multiple data segments and our main goal is to have the children nodes with maximum homogeneity or purity.3. Leaf nodes:Â These nodes represent the data section having the highest homogeneity.,different type node decision tree
381,What are the disadvantages of Information Gain?,"nformation gain is defined as the reduction in entropy due to the selection of a particular attribute. Information gain biases the Decision Tree against considering attributes with a large number of distinct values which might lead to overfitting.In order to solve this problem, theÂ Information Gain RatioÂ is used.",disadvantage information gain
382,Explain Feature Selection using the Information Gain/Entropy Technique,The goal of the feature selection while building a Decision Tree is to select features or attributes (Decision nodes) which lead to a split in children nodes whose combined entropy adds up to lower entropy than the entropy value of the data segment before the split. This implies higher information gain.,feature selection using information gainentropy technique
383,Are Decision Trees affected by the outliers? Explain.,"Decision Trees are not sensitive to noisy data or outliers since, extreme values or outliers, never cause much reduction inÂ Residual Sum of Squares(RSS),Â because they are never involved in the split.",decision tree affected outlier
384,List down the advantages of the Decision Trees,"1. Clear Visualization:  This algorithm is simple to understand, interpret and visualize as the idea is mostly used in our daily lives. The output of a Decision Tree can be easily interpreted by humans.

2. Simple and easy to understand: Decision Tree works in the same manner as simple if-else statements which are very easy to understand.

3. This can be used for both classification and regression problems.

4. Decision Trees can handle both continuous and categorical variables.

5. No feature scaling required: There is no requirement of feature scaling techniques such as standardization and normalization in the case of Decision Tree as it uses a rule-based approach instead of calculation of distances.

6. Handles nonlinear parameters efficiently: Unlike curve-based algorithms, the performance of decision trees canÂt be affected by the Non-linear parameters. So, if there is high non-linearity present between the independent variables, Decision Trees may outperform as compared to other curve-based algorithms.

7. Decision Tree can automatically handle missing values.

8. Decision Tree handles the outliers automatically, hence they are usually robust to outliers.

9. Less Training Period: The training period of decision trees is less as compared to ensemble techniques like Random Forest because it generates only one Tree unlike the forest of trees in the Random Forest.",advantage decision tree
385,List out the disadvantages of the Decision Trees.,"1. Overfitting: This is the major problem associated with the Decision Trees. It generally leads to overfitting of the data which ultimately leads to wrong predictions for testing data points. it keeps generating new nodes in order to fit the data including even noisy data and ultimately the Tree becomes too complex to interpret. In this way, it loses its generalization capabilities. Therefore, it performs well on the training dataset but starts making a lot of mistakes on the test dataset.

2. High variance: As mentioned, a Decision Tree generally leads to the overfitting of data. Due to the overfitting, there is more likely a chance of high variance in the output which leads to many errors in the final predictions and shows high inaccuracy in the results. So, in order to achieve zero bias (overfitting), it leads to high variance due to the bias-variance tradeoff.

3. Unstable: When we add new data points it can lead to regeneration of the overall Tree. Therefore, all nodes need to be recalculated and reconstructed.

4. Not suitable for large datasets: If the data size is large, then one single Tree may grow complex and lead to overfitting. So in this case, we should use Random Forest instead, an ensemble technique of a single Decision Tree.",disadvantage decision tree
386,What is a Decision Tree?,A decision tree is a tree in which every node specifies a test of some attribute of the data and each branch descending from that node corresponds to one of the possible values for this attribute.,decision tree
